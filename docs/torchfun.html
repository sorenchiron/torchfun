
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>torchfun package &#8212; torchfun 1.0.82 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">torchfun 1.0.82 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="torchfun-package">
<h1>torchfun package<a class="headerlink" href="#torchfun-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchfun.tools.html">torchfun.tools package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchfun.tools.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchfun.tools.html#module-torchfun.tools.imgformat">torchfun.tools.imgformat module</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchfun.tools.html#module-torchfun.tools">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-torchfun.datasets">
<span id="torchfun-datasets-module"></span><h2>torchfun.datasets module<a class="headerlink" href="#module-torchfun.datasets" title="Permalink to this headline">¶</a></h2>
<p>provide dataset related classes</p>
<dl class="class">
<dt id="torchfun.datasets.ApposeDataset">
<em class="property">class </em><code class="descclassname">torchfun.datasets.</code><code class="descname">ApposeDataset</code><span class="sig-paren">(</span><em>*datasets</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.ApposeDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
<p>make several dataset abreast, 
returning tuples of their corresponding elements.
Usage:</p>
<blockquote>
<div><p>dataset = ApposeDataset(dataset1,dataset2,…)</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.datasets.ApposeDataset.check_consistency">
<code class="descname">check_consistency</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.ApposeDataset.check_consistency" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.datasets.BatchQueue">
<em class="property">class </em><code class="descclassname">torchfun.datasets.</code><code class="descname">BatchQueue</code><span class="sig-paren">(</span><em>max_length=50</em>, <em>y_max=1.5</em>, <em>y_min=0.8</em>, <em>img_size=128</em>, <em>channels=3</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="torchfun.datasets.BatchQueue.get">
<code class="descname">get</code><span class="sig-paren">(</span><em>queue</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.get" title="Permalink to this definition">¶</a></dt>
<dd><p>input: List[np.vector]
make an np.ndarray with size of: all x columns</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.get_batch">
<code class="descname">get_batch</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.get_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>get a batch with batch size of self.max_length*2</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.get_fake">
<code class="descname">get_fake</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.get_fake" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.get_real">
<code class="descname">get_real</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.get_real" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.get_real_fake">
<code class="descname">get_real_fake</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.get_real_fake" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.init_with_data">
<code class="descname">init_with_data</code><span class="sig-paren">(</span><em>real</em>, <em>fake</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.init_with_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.init_with_generator">
<code class="descname">init_with_generator</code><span class="sig-paren">(</span><em>real_generator</em>, <em>base_generator</em>, <em>faking_model</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.init_with_generator" title="Permalink to this definition">¶</a></dt>
<dd><p>real will be marked as 1, 
fake samples generated by base_generator will be marked as 0</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.new_sample">
<code class="descname">new_sample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.new_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>returns one img or imgs according to generator’s batch size configuration</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.push">
<code class="descname">push</code><span class="sig-paren">(</span><em>real</em>, <em>fake</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.push" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.push_fake">
<code class="descname">push_fake</code><span class="sig-paren">(</span><em>fakeimg</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.push_fake" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.push_real">
<code class="descname">push_real</code><span class="sig-paren">(</span><em>realimg</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.push_real" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.datasets.BatchQueue.reinit">
<code class="descname">reinit</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.BatchQueue.reinit" title="Permalink to this definition">¶</a></dt>
<dd><p>init the queue again after parameters are loaded by restore()</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.datasets.ImageAugmentationDataset">
<em class="property">class </em><code class="descclassname">torchfun.datasets.</code><code class="descname">ImageAugmentationDataset</code><span class="sig-paren">(</span><em>root</em>, <em>pre_transform=None</em>, <em>degrade_transform=None</em>, <em>post_transform=None</em>, <em>loader=&lt;function default_loader&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.datasets.ImageAugmentationDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.datasets.folder.ImageFolder</span></code></p>
<p>Pre_transform is applied to the source image firstly.
Then, degrade_transform is applied to get degraded image (such as blurred image).
Finally, both the degraded imgs and the pre-transformed images are uniformly post-transformed.
Usually, the post-transforms are ToTensor() and Normalize()</p>
<dl class="simple">
<dt>Degrade transform should have two arguments as input:</dt><dd><p>1: input image to be degraded
2: Boolean input indicates if the degrading parameters should be returned.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torchfun.functional">
<span id="torchfun-functional-module"></span><h2>torchfun.functional module<a class="headerlink" href="#module-torchfun.functional" title="Permalink to this headline">¶</a></h2>
<p>mathematical functions</p>
<dl class="function">
<dt id="torchfun.functional.add_noise">
<code class="descclassname">torchfun.functional.</code><code class="descname">add_noise</code><span class="sig-paren">(</span><em>in_tensor</em>, <em>noise_type='normal'</em>, <em>noise_param=(0</em>, <em>1)</em>, <em>range_limit=(-1</em>, <em>1)</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.add_noise" title="Permalink to this definition">¶</a></dt>
<dd><p>Add noise to input tensor.
Noise type can be either <cite>normal</cite> or <cite>uniform</cite></p>
<blockquote>
<div><ul class="simple">
<li><p>for normal, (mean,std) is required as noise_param</p></li>
<li><p>for uniform (min,max) is required as noise_param</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>The range of the output tensor can be limited,</dt><dd><p>by giving <cite>range_limit</cite>:(min,max)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.clip">
<code class="descclassname">torchfun.functional.</code><code class="descname">clip</code><span class="sig-paren">(</span><em>in_tensor</em>, <em>max_or_min</em>, <em>min_or_max</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.clip" title="Permalink to this definition">¶</a></dt>
<dd><p>limit the values in in_tensor to be within [min,max].
values larger than max will be cut to max, respectively for mins.
the order of max/min doesn’t matter.
the operation is not in-place, that saves you alot troubles.
Notice: this clip is not inplace, and neither can pass backward derivatives</p>
<blockquote>
<div><p>when the values are clipped to min/max.</p>
</div></blockquote>
<dl class="simple">
<dt>Warning: When applied during loss-calculating in training, this clip will cause</dt><dd><p>gradient disapperance.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.clip_">
<code class="descclassname">torchfun.functional.</code><code class="descname">clip_</code><span class="sig-paren">(</span><em>in_tensor</em>, <em>max_or_min</em>, <em>min_or_max</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.clip_" title="Permalink to this definition">¶</a></dt>
<dd><p>limit the values in in_tensor to be within [min,max].
values larger than max will be cut to max, respectively for mins.
the order of max/min doesn’t matter.
the operation is not in-place, that saves you alot troubles.
Notice: this clip is not inplace.</p>
<blockquote>
<div><p>But, for clipped values, the gradients will always be passed backwards.
This is useful when you want <cite>clip</cite> as a value-normalizer, but don not want the gradient to vanish.</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.combine_parameters">
<code class="descclassname">torchfun.functional.</code><code class="descname">combine_parameters</code><span class="sig-paren">(</span><em>*models</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.combine_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine the parameters of serveral trainable module object,
into one unified parameter generator.
Arguements:</p>
<blockquote>
<div><p><a href="#id1"><span class="problematic" id="id2">*</span></a>models: any number of models.</p>
</div></blockquote>
<p>This utility is useful when you want several individual parts to be
handled by one Optimizer. Parameters shall be gathered into one iterator
first, because torch.optimizers require only one parameter-iterator as input</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.conv2d_dfs">
<code class="descclassname">torchfun.functional.</code><code class="descname">conv2d_dfs</code><span class="sig-paren">(</span><em>x</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.conv2d_dfs" title="Permalink to this definition">¶</a></dt>
<dd><p>depth fully shared conv2d.
Argument:</p>
<blockquote>
<div><p>x: input image with size: N x C x H x W
weight: shape shoule be : 1 x 1 x kernel-height x k-width
bias: contains only 1 number or None</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.cosine_similarity">
<code class="descclassname">torchfun.functional.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>compute cosine similarity between two batch of data.
x1 x2 are flattened first, into N x V shape.
then each paired sample between x1 and x2 are used to compute a 
multi-dimensional vectorial cosine similarity.</p>
<p>output:  similarity output with size N, which is a vector.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.flatten">
<code class="descclassname">torchfun.functional.</code><code class="descname">flatten</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flatten function
Usage:</p>
<blockquote>
<div><p>out = flatten(x)</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.instance_mean_std">
<code class="descclassname">torchfun.functional.</code><code class="descname">instance_mean_std</code><span class="sig-paren">(</span><em>x</em>, <em>num_features</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.instance_mean_std" title="Permalink to this definition">¶</a></dt>
<dd><p>NCHW</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.instance_renorm">
<code class="descclassname">torchfun.functional.</code><code class="descname">instance_renorm</code><span class="sig-paren">(</span><em>x</em>, <em>mean</em>, <em>std</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.instance_renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Make x have given mean and std.
Argument:
x: NCHW
mean: tensor with size: N-by-num-features
std: tensor with size: N-by-num-features
eps: default 1e-5</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.max_min_norm">
<code class="descclassname">torchfun.functional.</code><code class="descname">max_min_norm</code><span class="sig-paren">(</span><em>x</em>, <em>to_max</em>, <em>to_min</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.max_min_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>scale the input x into range [to_min,to_max].
Arguments:</p>
<blockquote>
<div><p>to_max: target expect max value of the output
to_min: target expect min value of the output</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.functional.subpixel">
<code class="descclassname">torchfun.functional.</code><code class="descname">subpixel</code><span class="sig-paren">(</span><em>x</em>, <em>out_channels=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.functional.subpixel" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfold channel/depth dimensions to enlarge the feature map
Notice: Output size is deducted. 
The size of the unfold square is automatically determined
e.g. :</p>
<blockquote>
<div><p>images: 100x9x16x16.  9=3x3 square
subpixel-out: 100x1x48x48</p>
</div></blockquote>
<dl class="simple">
<dt>Arguement:</dt><dd><p>x: NCHW image, channel first.
out_channels, channel number of output feature map</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torchfun.metrics">
<span id="torchfun-metrics-module"></span><h2>torchfun.metrics module<a class="headerlink" href="#module-torchfun.metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchfun.metrics.MS_SSIM">
<em class="property">class </em><code class="descclassname">torchfun.metrics.</code><code class="descname">MS_SSIM</code><span class="sig-paren">(</span><em>size_average=True</em>, <em>max_val=255</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.MS_SSIM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="torchfun.metrics.MS_SSIM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>img1</em>, <em>img2</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.MS_SSIM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torchfun.metrics.MS_SSIM.ms_ssim">
<code class="descname">ms_ssim</code><span class="sig-paren">(</span><em>img1</em>, <em>img2</em>, <em>levels=5</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.MS_SSIM.ms_ssim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.metrics.SSIM">
<em class="property">class </em><code class="descclassname">torchfun.metrics.</code><code class="descname">SSIM</code><span class="sig-paren">(</span><em>window_size=11</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.SSIM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>structural similarity</p>
<dl class="method">
<dt id="torchfun.metrics.SSIM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>img1</em>, <em>img2</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.SSIM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.metrics.create_window">
<code class="descclassname">torchfun.metrics.</code><code class="descname">create_window</code><span class="sig-paren">(</span><em>window_size</em>, <em>channel</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.create_window" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.metrics.gaussian">
<code class="descclassname">torchfun.metrics.</code><code class="descname">gaussian</code><span class="sig-paren">(</span><em>window_size</em>, <em>sigma</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.gaussian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.metrics.psnr">
<code class="descclassname">torchfun.metrics.</code><code class="descname">psnr</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>eps=1e-11</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.psnr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.metrics.ssim">
<code class="descclassname">torchfun.metrics.</code><code class="descname">ssim</code><span class="sig-paren">(</span><em>img1</em>, <em>img2</em>, <em>window_size=11</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.metrics.ssim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-torchfun.nn">
<span id="torchfun-nn-module"></span><h2>torchfun.nn module<a class="headerlink" href="#module-torchfun.nn" title="Permalink to this headline">¶</a></h2>
<p>Neural Network related layers/functions/classes
that are compatible with all pyTorch usage.</p>
<dl class="class">
<dt id="torchfun.nn.Clip">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Clip</code><span class="sig-paren">(</span><em>max_or_min</em>, <em>min_or_max</em>, <em>dtype=torch.float32</em>, <em>keep_grad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Clip" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>limit the values in in_tensor to be within [min,max].
values larger than max will be cut to max, respectively for mins.
the order of max/min doesn’t matter.
the operation is not in-place, that saves you alot troubles.
Notice: this clip is not inplace.</p>
<blockquote>
<div><p>But, for clipped values, the gradients will always be passed backwards.
This is useful when you want <cite>clip</cite> as a value-normalizer, but don not want the gradient to vanish.</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.nn.Clip.forward_clip_grad">
<code class="descname">forward_clip_grad</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Clip.forward_clip_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.nn.Clip.forward_keep_grad">
<code class="descname">forward_keep_grad</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Clip.forward_keep_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Conv2dDepthFullyShared">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Conv2dDepthFullyShared</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Conv2dDepthFullyShared" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Conv2dDepthShared">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Conv2dDepthShared</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>trunks</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Conv2dDepthShared" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
<p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>Share the kernel along depth/channel direction.
Conv2dDepthShared divides input images into multiple sub-layers(trunks) along depth axis, and use shared kernel to process each depth trunk.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*},\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels. 
The <cite>weight</cite> and <cite>bias</cite> matrix of a Conv2dDepthShared are low-rank. 
They share trunks of digits repeatitively inside their matrices.</p>
<dl class="simple">
<dt>Example:</dt><dd><p>Conv(in=3,out=9,k=3,s=1) will create kernel weight with size of (9x3x5x5)
kernel of a Depth-shared-Conv2d only has 3x1x5x5 parameters.</p>
</dd>
</dl>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p>In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math notranslate nohighlight">\((\text{in_channels}=C_{in}, \text{out_channels}=C_{in} * K, ..., \text{groups}=C_{in})\)</span></p>
</div>
<dl>
<dt>Args:</dt><dd><p>in_channels (int): Number of channels in the input image, inchannels must can be divided by trunks.
out_channels (int): Number of channels produced by the convolution. out channels must can be divided by trunks.
trunks (int): Number of trunks a single image is divided into (along depth). All trunks inside an image share same weight/bias.
kernel_size (int or tuple): Size of the convolving kernel
stride (int or tuple, optional): Stride of the convolution. Default: 1
padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
bias (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor\frac{H_{in}  + 2 * \text{padding}[0] - \text{dilation}[0]
          * (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in}  + 2 * \text{padding}[1] - \text{dilation}[1]
          * (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
<dt>Attributes:</dt><dd><dl class="simple">
<dt>weight (Tensor): the learnable weights of the module of shape</dt><dd><p>(out_channels, in_channels, kernel_size[0], kernel_size[1])</p>
</dd>
</dl>
<p>bias (Tensor):   the learnable bias of the module of shape (out_channels)</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchfun.nn.Conv2dDepthShared.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Conv2dDepthShared.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.DebugAgent">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">DebugAgent</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.DebugAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>wrapper around layers.
this wrapper hooks the forward function, to measure time 
consumption of this layer.</p>
<dl class="method">
<dt id="torchfun.nn.DebugAgent.bind">
<code class="descname">bind</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.DebugAgent.bind" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Flatten">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Flatten</code><a class="headerlink" href="#torchfun.nn.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Flatten module
Usage:</p>
<blockquote>
<div><p>flat = Flatten()
out = flat(x)</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.nn.Flatten.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Flatten.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.InstanceMeanStd">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">InstanceMeanStd</code><span class="sig-paren">(</span><em>num_features</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.InstanceMeanStd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>NCHW</p>
<dl class="method">
<dt id="torchfun.nn.InstanceMeanStd.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.InstanceMeanStd.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>NCHW</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.InstanceReNorm">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">InstanceReNorm</code><span class="sig-paren">(</span><em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.InstanceReNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Make x have given mean and std.
Argument:
x: NCHW
mean: tensor with size: N-by-num-features
std: tensor with size: N-by-num-features
eps: default 1e-5</p>
<dl class="method">
<dt id="torchfun.nn.InstanceReNorm.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>mean</em>, <em>std</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.InstanceReNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Make x have given mean and std.
Argument:
x: NCHW
mean: tensor with size: N-by-num-features
std: tensor with size: N-by-num-features
eps: default 1e-5</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Interpolate">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Interpolate</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em>, <em>mode='bilinear'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>resizing/scaling multi dimensional tensors
The modes available for resizing are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only), <cite>area</cite></p>
<dl>
<dt>Args:</dt><dd><p>input (Tensor): the input tensor
size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</p>
<blockquote>
<div><p>output spatial size.</p>
</div></blockquote>
<p>scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.
mode (string): algorithm used for upsampling:</p>
<blockquote>
<div><p>‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. Default: ‘nearest’</p>
</div></blockquote>
<dl class="simple">
<dt>align_corners (bool, optional): if True, the corner pixels of the input</dt><dd><p>and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code> for concrete examples on how this
affects the outputs.</p>
</div>
<dl class="method">
<dt id="torchfun.nn.Interpolate.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Interpolate.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.MaxMinNorm">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">MaxMinNorm</code><span class="sig-paren">(</span><em>max_or_min</em>, <em>min_or_max</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.MaxMinNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.nn.Clip" title="torchfun.nn.Clip"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.nn.Clip</span></code></a></p>
<p>scale the input x into range [to_min,to_max].
Arguments:</p>
<blockquote>
<div><p>to_max: target expect max value of the output
to_min: target expect min value of the output</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.nn.MaxMinNorm.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.MaxMinNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Module">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Module</code><a class="headerlink" href="#torchfun.nn.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>More debugging/controlling methods with complete original
features from torch.nn.Module
provides:</p>
<blockquote>
<div><ul class="simple">
<li><p>debug mode: inspect running time layer by layer.</p></li>
<li><p>release mode: back to normal from debug mode.</p></li>
<li><p>freeze layers: set layers to be in-trainable</p></li>
<li><p>unfreeze: as the name implies.</p></li>
<li><p>unfreeze_all: so as the name implies.</p></li>
</ul>
</div></blockquote>
<dl>
<dt>Notice: you can safely change the base class from torchfun module</dt><dd><p>back to torch module, when you want to publish the model.
the state_dict will be loaded correctly and the forward() will
function the same.</p>
</dd>
<dt>Hint:   Consider establishing a BaseClass global variable at the top of your </dt><dd><p>code. Use a argument parser to select between torchfun.nn.Module and torch.nn.Module,
so that the following classes follows the specified base class</p>
<dl>
<dt>Example::</dt><dd><div class="line-block">
<div class="line">import torch.nn.Module as ReleaseModule</div>
<div class="line">import torchfun.nn.Module as DevModule    </div>
<div class="line">from sys import argv</div>
<div class="line">if argv[1] == ‘develop’:</div>
<div class="line-block">
<div class="line">Base = DevModule</div>
</div>
<div class="line">elif argv[1] == ‘release’:</div>
<div class="line-block">
<div class="line">Base = ReleaseModule</div>
<div class="line"><br /></div>
</div>
<div class="line">class MyModel(Base):</div>
<div class="line-block">
<div class="line">…</div>
</div>
</div>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="torchfun.nn.Module.debug">
<code class="descname">debug</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.debug" title="Permalink to this definition">¶</a></dt>
<dd><p>turn on debug mode.
allow detailed timing report of forward()</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.nn.Module.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><em>*obj_or_name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.freeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.nn.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>Parameter: module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torchfun.nn.Module.release">
<code class="descname">release</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.release" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.nn.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torchfun.nn.Module.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><em>*obj_or_name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.nn.Module.unfreeze_all">
<code class="descname">unfreeze_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Module.unfreeze_all" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.NO_OP">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">NO_OP</code><span class="sig-paren">(</span><em>*argv</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.NO_OP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A Module that repersents NO-Operation NO-OP.
NO-OP is needed when programmers want customizable dynamic assembling
of models. 
To disable some layers, instead of using multiple <cite>if</cite> clauses, nn-parts can be configured to be
NO-OP class, which will make that part turned-off in all occurrence.</p>
<p>Notice: NO_OP will accept any init-args, and ignore them.</p>
<dl class="staticmethod">
<dt id="torchfun.nn.NO_OP.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>*argv</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.NO_OP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.ReLU">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>*args</em>, <em>inplace=False</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.ReLU</span></code></p>
<p>activation that accepts any argument and ignores them.
useful when you want to switch between different activations
programatically,</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.nn.ReLUwithGrad">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">ReLUwithGrad</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.ReLUwithGrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>a relu function with gradient not clipped to zero.
the gradient w.r.t input is not zero even when the input
is negative value.</p>
<p>This RELU output the value like a normal relu max(0,input)
but it output backward() like a linear function y=x,
dReLU/dx = 1</p>
<dl class="method">
<dt id="torchfun.nn.ReLUwithGrad.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.ReLUwithGrad.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Squeeze">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Squeeze</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>squeeze(dim=None) -&gt; Tensor</p>
<p>Returns a tensor with all the dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of size <cite>1</cite> removed.</p>
<p>For example, if <cite>input</cite> is of shape:
<span class="math notranslate nohighlight">\((A   imes 1  imes B  imes C  imes 1  imes D)\)</span> then the <cite>out</cite> tensor
will be of shape: <span class="math notranslate nohighlight">\((A         imes B  imes C  imes D)\)</span>.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is given, a squeeze operation is done only in the given
dimension. If <cite>input</cite> is of shape: <span class="math notranslate nohighlight">\((A        imes 1  imes B)\)</span>,
<cite>squeeze(input, 0)</cite> leaves the tensor unchanged, but <code class="xref py py-func docutils literal notranslate"><span class="pre">squeeze(input,</span> <span class="pre">1)()</span></code> will
squeeze the tensor to the shape <span class="math notranslate nohighlight">\((A   imes B)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As an exception to the above, a 1-dimensional tensor of size 1 will
not have its dimensions changed.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned tensor shares the storage with the input tensor,
so changing the contents of one will change the contents of the other.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 1, 2, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 2, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 1, 2, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 2, 1, 2]) </span>
</pre></div>
</div>
<dl class="method">
<dt id="torchfun.nn.Squeeze.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Squeeze.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.nn.Subpixel">
<em class="property">class </em><code class="descclassname">torchfun.nn.</code><code class="descname">Subpixel</code><span class="sig-paren">(</span><em>out_channels=1</em>, <em>stride=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Subpixel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Unfold channel/depth dimensions to enlarge the feature map
Notice: Output size is deducted. 
The size of the unfold square is automatically determined
e.g. :</p>
<blockquote>
<div><p>images: 100x16x16x9.  9=3x3 square
subpixel-out: 100x48x48x1</p>
</div></blockquote>
<dl class="simple">
<dt>Arguement:</dt><dd><p>out_channels, channel number of output feature map
stride: enlarging ratio of spatial dimensions. stride=2 outputs x4 img area. If provided, out_channels will be ignored.</p>
</dd>
</dl>
<dl class="method">
<dt id="torchfun.nn.Subpixel.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.nn.Subpixel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.nn.Module" title="torchfun.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torchfun.path">
<span id="torchfun-path-module"></span><h2>torchfun.path module<a class="headerlink" href="#module-torchfun.path" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torchfun.path.add_prefix">
<code class="descclassname">torchfun.path.</code><code class="descname">add_prefix</code><span class="sig-paren">(</span><em>path</em>, <em>prefix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.path.add_prefix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.path.add_tag">
<code class="descclassname">torchfun.path.</code><code class="descname">add_tag</code><span class="sig-paren">(</span><em>path</em>, <em>tag</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.path.add_tag" title="Permalink to this definition">¶</a></dt>
<dd><p>add tag to the end of the filename, before the suffix(file type mark)  
dir_path/filename{tag}.suffix</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.path.change_suffix">
<code class="descclassname">torchfun.path.</code><code class="descname">change_suffix</code><span class="sig-paren">(</span><em>path</em>, <em>suffix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.path.change_suffix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.path.get_dir_fname_suffix">
<code class="descclassname">torchfun.path.</code><code class="descname">get_dir_fname_suffix</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.path.get_dir_fname_suffix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-torchfun.torchfun">
<span id="torchfun-torchfun-module"></span><h2>torchfun.torchfun module<a class="headerlink" href="#module-torchfun.torchfun" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torchfun.torchfun.Options">
<em class="property">class </em><code class="descclassname">torchfun.torchfun.</code><code class="descname">Options</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Options" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
<p>A simple yet effective option class for debugging use.
key features: you can set attributes to it directly.
like:</p>
<blockquote>
<div><p>o = Options()
o.what=1
o.hahah=123</p>
</div></blockquote>
</dd></dl>

<dl class="class">
<dt id="torchfun.torchfun.Packsearch">
<em class="property">class </em><code class="descclassname">torchfun.torchfun.</code><code class="descname">Packsearch</code><span class="sig-paren">(</span><em>module_object</em>, <em>auto_init=True</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Packsearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Search names inside a package.</p>
<p>Given an module object as input:</p>
<p>&gt; p = Packsearch(torch)</p>
<p>or</p>
<p>&gt; p = Packsearch(‘torch’)</p>
<p>the instance p provide p.search() method. So that you can</p>
<p>search everything inside this package</p>
<p>&gt; p.search(‘maxpoo’)</p>
<p>or simply</p>
<p>&gt; p(‘maxpoo’)</p>
<p>output:</p>
<blockquote>
<div><p>Packsearch: 35 results found:</p>
<p>————-results start————-</p>
<p>0        torch.nn.AdaptiveMaxPool1d</p>
<p>1        torch.nn.AdaptiveMaxPool2d</p>
<p>2        torch.nn.AdaptiveMaxPool3d</p>
<p>3        torch.nn.FractionalMaxPool2d</p>
<p>4        torch.nn.MaxPool1d</p>
<p>5        torch.nn.MaxPool2d</p>
<p>…</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.torchfun.Packsearch.dynamic_traverse">
<code class="descname">dynamic_traverse</code><span class="sig-paren">(</span><em>mod</em>, <em>query</em>, <em>search_attributes=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Packsearch.dynamic_traverse" title="Permalink to this definition">¶</a></dt>
<dd><p>traverse the module and simultaneously search for queried name</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.torchfun.Packsearch.preprocess_names">
<code class="descname">preprocess_names</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Packsearch.preprocess_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.torchfun.Packsearch.search">
<code class="descname">search</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Packsearch.search" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.torchfun.Packsearch.traverse">
<code class="descname">traverse</code><span class="sig-paren">(</span><em>mod</em>, <em>search_attributes=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Packsearch.traverse" title="Permalink to this definition">¶</a></dt>
<dd><p>gather all names and store them into a name_list
search_attributes: whether to include class attributes or method names</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.torchfun.Unimodel">
<em class="property">class </em><code class="descclassname">torchfun.torchfun.</code><code class="descname">Unimodel</code><span class="sig-paren">(</span><em>*models</em>, <em>**named_models</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Unimodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>this class is used to gather your multiple models, so that
you can save/load them together.</p>
<p>usage:</p>
<blockquote>
<div><p>unimodel = Unimodel(resnet1,resnet2,resnet3)</p>
<p>unimodel.save(‘xxxx.pth’)</p>
<p>unimodel.load(‘ssss.pth’)</p>
<p>resnet1 = unimodel.resnet1</p>
<p>…</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.torchfun.Unimodel.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Unimodel.load" title="Permalink to this definition">¶</a></dt>
<dd><p>same as torchfun load()</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.torchfun.Unimodel.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.Unimodel.save" title="Permalink to this definition">¶</a></dt>
<dd><p>same as torchfun save()</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.count_parameters">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">count_parameters</code><span class="sig-paren">(</span><em>model_or_dict_or_param</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.count_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Count parameter numer of a module/state_dict/layer/tensor.</p>
<p>This function can also print the occupied memory of parameters in MBs</p>
<p>Arguements:</p>
<p>model_or_dict_or_param: model or state dictionary or model.parameter(), or numpy-array, or tensor.</p>
<p>Return: parameter amount in python-int</p>
<blockquote>
<div><p>Returns 0 if datatype not understood</p>
</div></blockquote>
<p>Usage:</p>
<ol class="arabic">
<li><p>count trainable and untrainbale params</p>
<blockquote>
<div><p>count_parameters(model)</p>
</div></blockquote>
</li>
</ol>
<p>same as</p>
<blockquote>
<div><p>count_parameters(state_dict)</p>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>count only trainable params:</p>
<blockquote>
<div><p>count_parameters(model.parameters())</p>
</div></blockquote>
</li>
<li><p>count data matrix</p>
<blockquote>
<div><p>count_parameters(weight_tensor)</p>
<p>count_parameters(numpy_array)</p>
</div></blockquote>
</li>
</ol>
<p>Notice:</p>
<blockquote>
<div><p>return value is parameter Number.</p>
</div></blockquote>
<p>Alias: parameters()</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.documentation">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">documentation</code><span class="sig-paren">(</span><em>search=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.documentation" title="Permalink to this definition">¶</a></dt>
<dd><p>help documentation on Torchfun
Argument:</p>
<blockquote>
<div><p>search: give None to go to the latest doc site</p>
<blockquote>
<div><p>give string or object to search the object</p>
</div></blockquote>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.dtype">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">dtype</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>return the data type of:</p>
<blockquote>
<div><p>a model</p>
<p>a tensor</p>
<p>or, the type() of anything else</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.hash_parameters">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">hash_parameters</code><span class="sig-paren">(</span><em>model_or_statdict_or_param</em>, <em>use_sum=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.hash_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>return the summary of all variables.
This is used to detect chaotic changes of weights.
You can check the sum_parameters before and after some operations, to know
if there is any change made to the params.</p>
<p>I use this function to verify gradient behaviours.</p>
<p>By default, This only hash the trainable parameters!</p>
<p>arguements:</p>
<p>module_or_statdict_or_param: torch.nn.module,</p>
<blockquote>
<div><p>or model.state_dict(),</p>
<p>or model.parameters().</p>
</div></blockquote>
<p>use_sum: return the sum instead of mean value of all params.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.imcrop_center">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">imcrop_center</code><span class="sig-paren">(</span><em>tensor_or_size</em>, <em>size_or_tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.imcrop_center" title="Permalink to this definition">¶</a></dt>
<dd><p>crop a center patch from pytorch image NCHW</p>
<p>arguments</p>
<blockquote>
<div><p>tensor: NCHW tensor</p>
<dl class="simple">
<dt>size: tuple or list of [height,width], or a scale factor</dt><dd><p>given a tuple of height,width, this returns a scaled patch of the 
largest center crop.
given a factor of scale, this will return a scaled square maximum center crop</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.imread">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">imread</code><span class="sig-paren">(</span><em>fname</em>, <em>out_range=(0</em>, <em>1)</em>, <em>dtype=torch.float32</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.imread" title="Permalink to this definition">¶</a></dt>
<dd><p>read jpg/png/gif/bmp/tiff… image file, and cat to tensor
function based on imageio.</p>
<p>args:</p>
<blockquote>
<div><p>fname: string of the file path</p>
<p>out_range: tuple, output pixel value range.</p>
<p>dtype: torch datatype</p>
</div></blockquote>
<dl class="simple">
<dt>Notice:</dt><dd><p>the returned image is 1xCxHxW (NCHW).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.imresize">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">imresize</code><span class="sig-paren">(</span><em>tensor_or_size</em>, <em>size_or_tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.imresize" title="Permalink to this definition">¶</a></dt>
<dd><p>stretch pytorch image NCHW, into given shape</p>
<p>arguments</p>
<blockquote>
<div><p>tensor: NCHW tensor</p>
<p>size: tuple or list of [height,width], or a scale factor</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.imsave">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">imsave</code><span class="sig-paren">(</span><em>img_or_dest</em>, <em>dest_or_img</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.imsave" title="Permalink to this definition">¶</a></dt>
<dd><p>save torch tensor image(s) or numpy image(s).</p>
<p>img: can be numpy image, numpy image batch.</p>
<blockquote>
<div><p>or single torch-tensor image</p>
<p>or torch-tensor-image-batch.</p>
<p>or list/tuple of numpy images</p>
<p>or list/tuple of pytorch-tensor images</p>
</div></blockquote>
<p>Notice: images must have non-zero channels, even for grey-scale images (1-channel images).</p>
<dl class="simple">
<dt>behaviour: images will be concatenated into a long single image and save into destination file-name.</dt><dd><p>the concating will be taken horizontally.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.imshow">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">imshow</code><span class="sig-paren">(</span><em>x</em>, <em>title=None</em>, <em>auto_close=True</em>, <em>cols=8</em>, <em>backend=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.imshow" title="Permalink to this definition">¶</a></dt>
<dd><p>only deal with torch channel-first image batch,</p>
<dl class="simple">
<dt>title: add title to plot. (Default None)</dt><dd><p>title can be string, or any string-able object.</p>
</dd>
<dt>auto_close: (default True) </dt><dd><p>Close the pyplot session afterwards. 
Clean the environment just like you had 
never used matplotlib here.
if set to False, the plot will remain in the memory for further drawings.</p>
</dd>
<dt>cols: columns(default 8)</dt><dd><p>the width of the output grid, aka, number of images per row.</p>
</dd>
<dt>backend: None to use default gui. options are:</dt><dd><p>WebAgg,  GTK3Agg,
WX,      GTK3Cairo,
WXAgg,   MacOSX,
WXCairo, nbAgg,
agg,     Qt4Agg,
cairo,   Qt4Cairo,
pdf,     Qt5Agg,
pgf,     Qt5Cairo,
ps,      TkAgg,
svg,     TkCairo,
template</p>
</dd>
</dl>
<p>Usage:</p>
<dl class="simple">
<dt><a href="#id3"><span class="problematic" id="id4">``</span></a><a href="#id5"><span class="problematic" id="id6">`</span></a>python</dt><dd><p>imshow(batch)
imshow(batch,title=[a,b,c])
imshow(batch,title=’title’)
imshow(batch,auto_close=False)</p>
</dd>
</dl>
<p><a href="#id7"><span class="problematic" id="id8">``</span></a><a href="#id9"><span class="problematic" id="id10">`</span></a></p>
<p>Warnings:
<a href="#id11"><span class="problematic" id="id12">``</span></a><a href="#id13"><span class="problematic" id="id14">`</span></a>text</p>
<blockquote>
<div><p>TorchFun:imshow:Warning, you are using WebAgg backend for Matplotlib. 
Please consider windowed display SDKs such as TkAgg backend and GTK* backends.</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">This</span> <span class="pre">means</span> <span class="pre">your</span> <span class="pre">matplotlib</span> <span class="pre">is</span> <span class="pre">using</span> <span class="pre">web-browser</span> <span class="pre">for</span> <span class="pre">figure</span> <span class="pre">display.</span> <span class="pre">We</span> <span class="pre">__strongly__</span> <span class="pre">recommend</span> <span class="pre">you</span> <span class="pre">to</span> <span class="pre">use</span> <span class="pre">window-based</span> <span class="pre">native</span> <span class="pre">display</span> <span class="pre">because</span> <span class="pre">browser-based</span> <span class="pre">backends</span> <span class="pre">are</span> <span class="pre">fragile</span> <span class="pre">and</span> <span class="pre">tend</span> <span class="pre">to</span> <span class="pre">crash.</span> <span class="pre">You</span> <span class="pre">can</span> <span class="pre">change</span> <span class="pre">the</span> <span class="pre">display</span> <span class="pre">mamanger</span> <span class="pre">for</span> <span class="pre">matplotlib</span> <span class="pre">each</span> <span class="pre">time</span> <span class="pre">you</span> <span class="pre">execute</span> <span class="pre">your</span> <span class="pre">script</span> <span class="pre">by:</span>
<span class="pre">```python</span>
<span class="pre">import</span> <span class="pre">matplotlib</span>
<span class="pre">matplotlib.use('TkAgg')</span> <span class="pre">#</span> <span class="pre">or</span> <span class="pre">GTK</span> <span class="pre">GTKAgg</span>
<span class="pre">`</span></code>
or permanantly by editing: <cite>site-packages/matplotlib/mpl-data/matplotlibrc</cite> and change backend to <cite>TkAgg</cite></p>
<p>If your are using Unix-like systems such as MacOSX, you can create ~/.matplotlib/matplotlibrc and add a line: <cite>backend:TkAgg</cite> to it.</p>
<p>A full list of available backends can be found at:
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">import</span> <span class="pre">matplotlib</span>
<span class="pre">matplotlib.rcsetup.all_backends</span>
<span class="pre">`</span></code>
and, the TCL/TK GUI library for <cite>tkinter</cite> can be downloaded [here](<a class="reference external" href="https://www.tcl.tk/">https://www.tcl.tk/</a>).</p>
<dl>
<dt>Notice:</dt><dd><p>If you use conda to manage your python versions, errors may occur when using TCL/TK.
That’s because conda secretly redirect your global python library path towards its.
That will cause other stand-alone python versions to search from conda’s lib dirs for binaries.
To solve this, you may have to set:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">export</span> <span class="pre">TCL_LIBRARY=/usr/...pythondir.../lib/tcl8.6</span>
<span class="pre">export</span> <span class="pre">TK_LIBRARY=/usr/...pythondir.../lib/tcl8.6</span>
<span class="pre">`</span></code>
or on windows:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">set</span> <span class="pre">&quot;TCL_LIBRARY=/usr/...pythondir.../lib/tcl8.6&quot;</span>
<span class="pre">set</span> <span class="pre">&quot;TK_LIBRARY=/usr/...pythondir.../lib/tcl8.6&quot;</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.imwrite">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">imwrite</code><span class="sig-paren">(</span><em>img_or_dest</em>, <em>dest_or_img</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.imwrite" title="Permalink to this definition">¶</a></dt>
<dd><p>save torch tensor image(s) or numpy image(s).</p>
<p>img: can be numpy image, numpy image batch.</p>
<blockquote>
<div><p>or single torch-tensor image</p>
<p>or torch-tensor-image-batch.</p>
<p>or list/tuple of numpy images</p>
<p>or list/tuple of pytorch-tensor images</p>
</div></blockquote>
<p>Notice: images must have non-zero channels, even for grey-scale images (1-channel images).</p>
<dl class="simple">
<dt>behaviour: images will be concatenated into a long single image and save into destination file-name.</dt><dd><p>the concating will be taken horizontally.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.load">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">load</code><span class="sig-paren">(</span><em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load weight <cite>a</cite> into model <cite>b</cite>, or load model <cite>b</cite> using weight <cite>a</cite>
The order of the arguments doesn’t matter.</p>
<p>Example:</p>
<blockquote>
<div><p>&gt;load(‘weights.pts’,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;load(model,’weights.pts’)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;f = open(‘weight.pts’)</p>
<p>&gt;load(f,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;load(model,f)</p>
</div></blockquote>
<p>Return value: None</p>
<dl class="simple">
<dt>Behaviour: the loaded state-dict will be transformed to the same device as model,</dt><dd><p>so that torch won’t complain about CUDA-memory-insufficient when you just want to load
weights from disk directly to cpu-model.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.options">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">options</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.options" title="Permalink to this definition">¶</a></dt>
<dd><p>warpping class for Options. this function returns an option object with attributes
set according to the input key-value arguments. 
please refer to Option class for more information</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.packsearch">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">packsearch</code><span class="sig-paren">(</span><em>module_or_str</em>, <em>str_or_module</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.packsearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an module object, and search pattern string as input:</p>
<p>&gt; packsearch(torch,’maxpoo’)</p>
<p>or</p>
<p>&gt; packsearch(‘maxpoo’,torch)</p>
<p>you can search everything inside this package</p>
<p>output:</p>
<blockquote>
<div><p>Packsearch: 35 results found:</p>
<p>————-results start————-</p>
<p>0        torch.nn.AdaptiveMaxPool1d</p>
<p>1        torch.nn.AdaptiveMaxPool2d</p>
<p>2        torch.nn.AdaptiveMaxPool3d</p>
<p>3        torch.nn.FractionalMaxPool2d</p>
<p>4        torch.nn.MaxPool1d</p>
<p>5        torch.nn.MaxPool2d</p>
<p>…</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.parameters">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">parameters</code><span class="sig-paren">(</span><em>model_or_dict_or_param</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Count parameter numer of a module/state_dict/layer/tensor.</p>
<p>This function can also print the occupied memory of parameters in MBs</p>
<p>Arguements:</p>
<p>model_or_dict_or_param: model or state dictionary or model.parameter(), or numpy-array, or tensor.</p>
<p>Return: parameter amount in python-int</p>
<blockquote>
<div><p>Returns 0 if datatype not understood</p>
</div></blockquote>
<p>Usage:</p>
<ol class="arabic">
<li><p>count trainable and untrainbale params</p>
<blockquote>
<div><p>count_parameters(model)</p>
</div></blockquote>
</li>
</ol>
<p>same as</p>
<blockquote>
<div><p>count_parameters(state_dict)</p>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>count only trainable params:</p>
<blockquote>
<div><p>count_parameters(model.parameters())</p>
</div></blockquote>
</li>
<li><p>count data matrix</p>
<blockquote>
<div><p>count_parameters(weight_tensor)</p>
<p>count_parameters(numpy_array)</p>
</div></blockquote>
</li>
</ol>
<p>Notice:</p>
<blockquote>
<div><p>return value is parameter Number.</p>
</div></blockquote>
<p>Alias: parameters()</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.pil_imshow">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">pil_imshow</code><span class="sig-paren">(</span><em>arr</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.pil_imshow" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple showing of an image through an external viewer.</p>
<p>This function is only available if Python Imaging Library (PIL) is installed.</p>
<p>Uses the image viewer specified by the environment variable
SCIPY_PIL_IMAGE_VIEWER, or if that is not defined then <cite>see</cite>,
to view a temporary file generated from array data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function uses <cite>bytescale</cite> under the hood to rescale images to use
the full (0, 255) range if <code class="docutils literal notranslate"><span class="pre">mode</span></code> is one of <code class="docutils literal notranslate"><span class="pre">None,</span> <span class="pre">'L',</span> <span class="pre">'P',</span> <span class="pre">'l'</span></code>.
It will also cast data for 2-D images to <code class="docutils literal notranslate"><span class="pre">uint32</span></code> for <code class="docutils literal notranslate"><span class="pre">mode=None</span></code>
(which is the default).</p>
</div>
<dl class="simple">
<dt>arr<span class="classifier">ndarray</span></dt><dd><p>Array of image data to show.</p>
</dd>
</dl>
<p>None</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">misc</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">misc</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<p>Ported and upgraded based on scipy.misc.imshow
Open-sourced according to the license.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.save">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">save</code><span class="sig-paren">(</span><em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.save" title="Permalink to this definition">¶</a></dt>
<dd><p>save weight <cite>a</cite> into target <cite>b</cite>, or save model <cite>b</cite> into target <cite>a</cite>
The order of the arguments doesn’t matter.
Example:</p>
<blockquote>
<div><p>&gt;save(‘weights.pts’,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(model,’weights.pts’)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;f = open(‘weight.pts’)
&gt;save(f,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(model,f)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(‘weights.pts’,state_dict)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(state_dict,’weights.pts’)</p>
</div></blockquote>
<p>Return value: None</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.show">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">show</code><span class="sig-paren">(</span><em>net</em>, <em>input_shape=(1</em>, <em>3</em>, <em>32</em>, <em>32)</em>, <em>logdir='tensorboard'</em>, <em>port=8888</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.show" title="Permalink to this definition">¶</a></dt>
<dd><p>print the network architecture on web-browser, using tensorboardX and tensorboard.
tensoboard must be install to use this tool.
this tool will create a noise data according to given input_shape,
and feed it directly to net, in order to probe its structure.
network strctures descriptions will be written to logdir.
a tensorboard daemon will be launched to read the logdir and start a web server
on given port.</p>
<p>Notice:</p>
<blockquote>
<div><p>input shape must be NCHW, following pytorch style.</p>
<p>This program overwrites the system argument lists (sys.argv)</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.show_layers_parameters">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">show_layers_parameters</code><span class="sig-paren">(</span><em>model</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.show_layers_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.tf_session">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">tf_session</code><span class="sig-paren">(</span><em>allow_growth=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.tf_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to create tensorflow session that does not stupidly and unresonably consume all gpu-memeory.
returns:</p>
<blockquote>
<div><p>a tensorflow session consuming dynamic gpu memory.</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.to_numpy">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">to_numpy</code><span class="sig-paren">(</span><em>tensor</em>, <em>keep_dim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.to_numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>convert a NCHW tensor to NHWC numpy array.
if the input tensor has only one image,
then the N dimension will be deleted.</p>
<p>if the input is not 4-dimensional, 
the tensor will be simply cat to np array</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.vectorize_parameter">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">vectorize_parameter</code><span class="sig-paren">(</span><em>model_or_statdict_or_param</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.vectorize_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>return the vectorized form of all variables.
This is used to detect chaotic changes of weights.</p>
<p>arguements:</p>
<p>module_or_statdict_or_param: torch.nn.module,</p>
<blockquote>
<div><p>or model.state_dict(),</p>
<p>or model.parameters().</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.torchfun.whereis">
<code class="descclassname">torchfun.torchfun.</code><code class="descname">whereis</code><span class="sig-paren">(</span><em>module_or_string</em>, <em>open_gui=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.torchfun.whereis" title="Permalink to this definition">¶</a></dt>
<dd><p>find the source file location of a module</p>
<p>arguments:</p>
<blockquote>
<div><p>module_or_string: target module object, or it’s string path like <cite>torch.nn</cite></p>
<p>open_gui: open the folder with default window-manager.</p>
</div></blockquote>
<p>returns:</p>
<blockquote>
<div><p>module file name, or None</p>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="module-torchfun.transforms">
<span id="torchfun-transforms-module"></span><h2>torchfun.transforms module<a class="headerlink" href="#module-torchfun.transforms" title="Permalink to this headline">¶</a></h2>
<p>Transforms used for data augmentation in torchvision.datasets.*
or in torch.utils.data.Dataset</p>
<p>New function: most of the transforms support x-y paired transformation.
New function: a place-holder base class Transform is created and used as the</p>
<blockquote>
<div><p>base class of all transforms. 
This change is made so that more complicated metatype-programming 
can be possible.</p>
</div></blockquote>
<p>Some of the code is borrowed from an old project but the original author is not found.</p>
<p>Please let me know if any part of this code is from your contribution.</p>
<dl class="class">
<dt id="torchfun.transforms.Transform">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">Transform</code><a class="headerlink" href="#torchfun.transforms.Transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomGaussianBlur">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomGaussianBlur</code><span class="sig-paren">(</span><em>kernel_ratio=0.01</em>, <em>random_ratio=0.005</em>, <em>pixel_range=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomGaussianBlur" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>PIL image</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.NoTransform">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">NoTransform</code><span class="sig-paren">(</span><em>*rubbish_args</em>, <em>**rubbish_kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.NoTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.Normalize">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">Normalize</code><span class="sig-paren">(</span><em>mean</em>, <em>std</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.Normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Normalize a tensor image with mean and standard deviation.
Given mean: <code class="docutils literal notranslate"><span class="pre">(M1,...,Mn)</span></code> and std: <code class="docutils literal notranslate"><span class="pre">(S1,..,Sn)</span></code> for <code class="docutils literal notranslate"><span class="pre">n</span></code> channels, this transform
will normalize each channel of the input <code class="docutils literal notranslate"><span class="pre">torch.*Tensor</span></code> i.e.
<code class="docutils literal notranslate"><span class="pre">input[channel]</span> <span class="pre">=</span> <span class="pre">(input[channel]</span> <span class="pre">-</span> <span class="pre">mean[channel])</span> <span class="pre">/</span> <span class="pre">std[channel]</span></code></p>
<dl class="simple">
<dt>Args:</dt><dd><p>mean (sequence): Sequence of means for each channel.
std (sequence): Sequence of standard deviations for each channel.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.Resize">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">Resize</code><span class="sig-paren">(</span><em>size</em>, <em>interpolation=2</em>, <em>interpolation_tg=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.Resize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Resize the input PIL Image to the given size.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size. If size is a sequence like</dt><dd><p>(h, w), output size will be matched to this. If size is an int,
smaller edge of the image will be matched to this number.
i.e, if height &gt; width, then image will be rescaled to
(size * height / width, size)</p>
</dd>
<dt>interpolation (int, optional): Desired interpolation. Default is</dt><dd><p><code class="docutils literal notranslate"><span class="pre">PIL.Image.BILINEAR</span></code></p>
</dd>
<dt>interpolation_tg (int, optional): Desired interpolation for target. Default is</dt><dd><p><code class="docutils literal notranslate"><span class="pre">PIL.Image.NEAREST</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.CenterCrop">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">CenterCrop</code><span class="sig-paren">(</span><em>size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.CenterCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crops the given PIL Image at the center.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an</dt><dd><p>int instead of sequence like (h, w), a square crop (size, size) is
made.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomCrop">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomCrop</code><span class="sig-paren">(</span><em>size</em>, <em>padding=0</em>, <em>pad_if_needed=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image at a random location.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an</dt><dd><p>int instead of sequence like (h, w), a square crop (size, size) is
made.</p>
</dd>
<dt>padding (int or sequence, optional): Optional padding on each border</dt><dd><p>of the image. Default is 0, i.e no padding. If a sequence of length
4 is provided, it is used to pad left, top, right, bottom borders
respectively.</p>
</dd>
<dt>pad_if_needed (boolean): It will pad the image if smaller than the</dt><dd><p>desired size to avoid raising an exception.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.transforms.RandomCrop.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>img</em>, <em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomCrop.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for <code class="docutils literal notranslate"><span class="pre">crop</span></code> for a random crop.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>img (PIL Image): Image to be cropped.
output_size (tuple): Expected output size of the crop.</p>
</dd>
<dt>Returns:</dt><dd><p>tuple: params (i, j, h, w) to be passed to <code class="docutils literal notranslate"><span class="pre">crop</span></code> for random crop.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomHorizontalFlip">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomHorizontalFlip</code><span class="sig-paren">(</span><em>p=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomHorizontalFlip" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Horizontally flip the given PIL Image randomly with a given probability.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p (float): probability of the image being flipped. Default value is 0.5</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomVerticalFlip">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomVerticalFlip</code><span class="sig-paren">(</span><em>p=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomVerticalFlip" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Vertically flip the given PIL Image randomly with a given probability.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p (float): probability of the image being flipped. Default value is 0.5</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomResizedCrop">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomResizedCrop</code><span class="sig-paren">(</span><em>size</em>, <em>scale=(0.08</em>, <em>1.0)</em>, <em>ratio=(0.75</em>, <em>1.3333333333333333)</em>, <em>interpolation=2</em>, <em>interpolation_tg=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomResizedCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image to random size and aspect ratio.</p>
<p>A crop of random size (default: of 0.08 to 1.0) of the original size and a random
aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop
is finally resized to given size.
This is popularly used to train the Inception networks.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>size: expected output size of each edge
scale: range of size of the origin size cropped
ratio: range of aspect ratio of the origin aspect ratio cropped
interpolation: Default: PIL.Image.BILINEAR</p>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.transforms.RandomResizedCrop.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>img</em>, <em>scale</em>, <em>ratio</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomResizedCrop.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for <code class="docutils literal notranslate"><span class="pre">crop</span></code> for a random sized crop.
Args:</p>
<blockquote>
<div><p>img (PIL Image): Image to be cropped.
scale (tuple): range of size of the origin size cropped
ratio (tuple): range of aspect ratio of the origin aspect ratio cropped</p>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt>tuple: params (i, j, h, w) to be passed to <code class="docutils literal notranslate"><span class="pre">crop</span></code> for a random</dt><dd><p>sized crop.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.FiveCrop">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">FiveCrop</code><span class="sig-paren">(</span><em>size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.FiveCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image into four corners and the central crop</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This transform returns a tuple of images and there may be a mismatch in the number of
inputs and targets your Dataset returns. See below for an example of how to deal with
this.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an <code class="docutils literal notranslate"><span class="pre">int</span></code></dt><dd><p>instead of sequence like (h, w), a square crop of size (size, size) is made.</p>
</dd>
</dl>
</dd>
<dt>Example:</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">FiveCrop</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="c1"># this is a list of PIL Images</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">crops</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">crop</span><span class="p">)</span> <span class="k">for</span> <span class="n">crop</span> <span class="ow">in</span> <span class="n">crops</span><span class="p">]))</span> <span class="c1"># returns a 4D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#In your test loop you can do the following:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span> <span class="c1"># input is a 5d tensor, target is 2d</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span> <span class="c1"># fuse batch size and ncrops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result_avg</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># avg over crops</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.TenCrop">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">TenCrop</code><span class="sig-paren">(</span><em>size</em>, <em>vertical_flip=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.TenCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image into four corners and the central crop plus the flipped version of
these (horizontal flipping is used by default)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This transform returns a tuple of images and there may be a mismatch in the number of
inputs and targets your Dataset returns. See below for an example of how to deal with
this.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an</dt><dd><p>int instead of sequence like (h, w), a square crop (size, size) is
made.</p>
</dd>
</dl>
<p>vertical_flip(bool): Use vertical flipping instead of horizontal</p>
</dd>
<dt>Example:</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">TenCrop</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="c1"># this is a list of PIL Images</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">crops</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">crop</span><span class="p">)</span> <span class="k">for</span> <span class="n">crop</span> <span class="ow">in</span> <span class="n">crops</span><span class="p">]))</span> <span class="c1"># returns a 4D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#In your test loop you can do the following:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span> <span class="c1"># input is a 5d tensor, target is 2d</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span> <span class="c1"># fuse batch size and ncrops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result_avg</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># avg over crops</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.LinearTransformation">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">LinearTransformation</code><span class="sig-paren">(</span><em>transformation_matrix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.LinearTransformation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Transform a tensor image with a square transformation matrix computed
offline.</p>
<p>Given transformation_matrix, will flatten the torch.*Tensor, compute the dot
product with the transformation matrix and reshape the tensor to its
original shape.</p>
<p>Applications:
- whitening: zero-center the data, compute the data covariance matrix</p>
<blockquote>
<div><p>[D x D] with np.dot(X.T, X), perform SVD on this matrix and
pass it as transformation_matrix.</p>
</div></blockquote>
<dl class="simple">
<dt>Args:</dt><dd><p>transformation_matrix (Tensor): tensor [D x D], D = C x H x W</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.ColorJitter">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">ColorJitter</code><span class="sig-paren">(</span><em>brightness=0</em>, <em>contrast=0</em>, <em>saturation=0</em>, <em>hue=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.ColorJitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Randomly change the brightness, contrast and saturation of an image.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>brightness (float): How much to jitter brightness. brightness_factor</dt><dd><p>is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].</p>
</dd>
<dt>contrast (float): How much to jitter contrast. contrast_factor</dt><dd><p>is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].</p>
</dd>
<dt>saturation (float): How much to jitter saturation. saturation_factor</dt><dd><p>is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].</p>
</dd>
<dt>hue(float): How much to jitter hue. hue_factor is chosen uniformly from</dt><dd><p>[-hue, hue]. Should be &gt;=0 and &lt;= 0.5.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.transforms.ColorJitter.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>brightness</em>, <em>contrast</em>, <em>saturation</em>, <em>hue</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.ColorJitter.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a randomized transform to be applied on image.</p>
<p>Arguments are same as that of __init__.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Transform which randomly adjusts brightness, contrast and
saturation in a random order.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomRotation">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomRotation</code><span class="sig-paren">(</span><em>degrees</em>, <em>resample=False</em>, <em>resample_tg=False</em>, <em>expand=False</em>, <em>center=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomRotation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Rotate the image by angle.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>degrees (sequence or float or int): Range of degrees to select from.</dt><dd><p>If degrees is a number instead of sequence like (min, max), the range of degrees
will be (-degrees, +degrees).</p>
</dd>
<dt>resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):</dt><dd><p>An optional resampling filter.
See <a class="reference external" href="http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters">http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters</a>
If omitted, or if the image has mode “1” or “P”, it is set to PIL.Image.NEAREST.</p>
</dd>
<dt>expand (bool, optional): Optional expansion flag.</dt><dd><p>If true, expands the output to make it large enough to hold the entire rotated image.
If false or omitted, make the output image the same size as the input image.
Note that the expand flag assumes rotation around the center and no translation.</p>
</dd>
<dt>center (2-tuple, optional): Optional center of rotation.</dt><dd><p>Origin is the upper left corner.
Default is the center of the image.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.transforms.RandomRotation.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>degrees</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomRotation.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for <code class="docutils literal notranslate"><span class="pre">rotate</span></code> for a random rotation.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>sequence: params to be passed to <code class="docutils literal notranslate"><span class="pre">rotate</span></code> for random rotation.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomAffine">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomAffine</code><span class="sig-paren">(</span><em>degrees</em>, <em>translate=None</em>, <em>scale=None</em>, <em>shear=None</em>, <em>resample=False</em>, <em>resample_tg=False</em>, <em>fillcolor=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomAffine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Random affine transformation of the image keeping center invariant</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>degrees (sequence or float or int): Range of degrees to select from.</dt><dd><p>If degrees is a number instead of sequence like (min, max), the range of degrees
will be (-degrees, +degrees). Set to 0 to desactivate rotations.</p>
</dd>
<dt>translate (tuple, optional): tuple of maximum absolute fraction for horizontal</dt><dd><p>and vertical translations. For example translate=(a, b), then horizontal shift
is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a and vertical shift is
randomly sampled in the range -img_height * b &lt; dy &lt; img_height * b. Will not translate by default.</p>
</dd>
<dt>scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is</dt><dd><p>randomly sampled from the range a &lt;= scale &lt;= b. Will keep original scale by default.</p>
</dd>
<dt>shear (sequence or float or int, optional): Range of degrees to select from.</dt><dd><p>If degrees is a number instead of sequence like (min, max), the range of degrees
will be (-degrees, +degrees). Will not apply shear by default</p>
</dd>
<dt>resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):</dt><dd><p>An optional resampling filter.
See <a class="reference external" href="http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters">http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters</a>
If omitted, or if the image has mode “1” or “P”, it is set to PIL.Image.NEAREST.</p>
</dd>
</dl>
<p>fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow&gt;=5.0.0)</p>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.transforms.RandomAffine.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>degrees</em>, <em>translate</em>, <em>scale_ranges</em>, <em>shears</em>, <em>img_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomAffine.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for affine transformation</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>sequence: params to be passed to the affine transformation</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.Grayscale">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">Grayscale</code><span class="sig-paren">(</span><em>num_output_channels=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.Grayscale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Convert image to grayscale.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>num_output_channels (int): (1 or 3) number of channels desired for output image</p>
</dd>
<dt>Returns:</dt><dd><p>PIL Image: Grayscale version of the input.
- If num_output_channels == 1 : returned image is single channel
- If num_output_channels == 3 : returned image is 3 channel with r == g == b</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.transforms.RandomGrayscale">
<em class="property">class </em><code class="descclassname">torchfun.transforms.</code><code class="descname">RandomGrayscale</code><span class="sig-paren">(</span><em>p=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.transforms.RandomGrayscale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Randomly convert image to grayscale with a probability of p (default 0.1).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p (float): probability that image should be converted to grayscale.</p>
</dd>
<dt>Returns:</dt><dd><p>PIL Image: Grayscale version of the input image with probability p and unchanged
with probability (1-p).
- If input image is 1 channel: grayscale version is 1 channel
- If input image is 3 channel: grayscale version is 3 channel with r == g == b</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torchfun.types">
<span id="torchfun-types-module"></span><h2>torchfun.types module<a class="headerlink" href="#module-torchfun.types" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torchfun.types.Bool">
<code class="descclassname">torchfun.types.</code><code class="descname">Bool</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.Bool" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=torchfun.bool),
in order to parse bool switch values like:
false False true True 0 1</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.types.TorchEasyList">
<em class="property">class </em><code class="descclassname">torchfun.types.</code><code class="descname">TorchEasyList</code><a class="headerlink" href="#torchfun.types.TorchEasyList" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p>
<dl class="method">
<dt id="torchfun.types.TorchEasyList.cat">
<code class="descname">cat</code><span class="sig-paren">(</span><em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.TorchEasyList.cat" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.types.TorchEasyList.push_back">
<code class="descname">push_back</code><span class="sig-paren">(</span><em>element</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.TorchEasyList.push_back" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.types.TorchEasyList.push_head">
<code class="descname">push_head</code><span class="sig-paren">(</span><em>element</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.TorchEasyList.push_head" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.types.argparse_bool_type">
<code class="descclassname">torchfun.types.</code><code class="descname">argparse_bool_type</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.argparse_bool_type" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=torchfun.bool),
in order to parse bool switch values like:
false False true True 0 1</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.types.argparse_list_type">
<code class="descclassname">torchfun.types.</code><code class="descname">argparse_list_type</code><span class="sig-paren">(</span><em>element_type=&lt;class 'int'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.argparse_list_type" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=?),
in order to parse lists like:
–gpu-ids=1,2,3,4</p>
<dl class="simple">
<dt>usage:</dt><dd><p>floatlist = argparse_list_type(float)
parser.add_argument(‘–cuda-devs’,type=flaotlist,default=[1,2])</p>
</dd>
<dt>args:</dt><dd><dl class="simple">
<dt>element_type: type of each element in the list,</dt><dd><p>eache element will be cat to this type.</p>
</dd>
</dl>
</dd>
</dl>
<p>Notice: Instances must be used, instead of this class itself.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.types.bool">
<code class="descclassname">torchfun.types.</code><code class="descname">bool</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.bool" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=torchfun.bool),
in order to parse bool switch values like:
false False true True 0 1</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.types.list_of_float">
<code class="descclassname">torchfun.types.</code><code class="descname">list_of_float</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.list_of_float" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.types.list_of_int">
<code class="descclassname">torchfun.types.</code><code class="descname">list_of_int</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.types.list_of_int" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-torchfun.ui">
<span id="torchfun-ui-module"></span><h2>torchfun.ui module<a class="headerlink" href="#module-torchfun.ui" title="Permalink to this headline">¶</a></h2>
<p>tools for user-interface interactions. 
(mostly for console-debugging)</p>
<dl class="function">
<dt id="torchfun.ui.error">
<code class="descclassname">torchfun.ui.</code><code class="descname">error</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ui.error" title="Permalink to this definition">¶</a></dt>
<dd><p>throw error and exit the program</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.ui.input_or">
<code class="descclassname">torchfun.ui.</code><code class="descname">input_or</code><span class="sig-paren">(</span><em>prompt</em>, <em>default='y'</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ui.input_or" title="Permalink to this definition">¶</a></dt>
<dd><p>get input from command-line,
if  the input is ommited by an enter key, then the default values is returned.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.ui.wait">
<code class="descclassname">torchfun.ui.</code><code class="descname">wait</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ui.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>wait and show a message, until a key is stroked.</p>
</dd></dl>

</div>
<div class="section" id="module-torchfun.utils">
<span id="torchfun-utils-module"></span><h2>torchfun.utils module<a class="headerlink" href="#module-torchfun.utils" title="Permalink to this headline">¶</a></h2>
<p>utilities for pytorch experiment. Non-mathematical tools.</p>
<dl class="function">
<dt id="torchfun.utils.change_fname">
<code class="descclassname">torchfun.utils.</code><code class="descname">change_fname</code><span class="sig-paren">(</span><em>fpath</em>, <em>new_fname</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.change_fname" title="Permalink to this definition">¶</a></dt>
<dd><p>change the name of file in the given fpath.
the suffix (if exists) separated by <cite>.</cite> will be kept.</p>
<p>In : change_fname(‘xxx/ddd/www’,’aa’)
Out: ‘xxxdddaa’</p>
<p>In : change_fname(‘xxx/ddd/www.txt’,’aa’)
Out: ‘xxxdddaa.txt’</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.change_suffix">
<code class="descclassname">torchfun.utils.</code><code class="descname">change_suffix</code><span class="sig-paren">(</span><em>fpath</em>, <em>new_suffix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.change_suffix" title="Permalink to this definition">¶</a></dt>
<dd><p>change the suffix of fpath to new_suffix.
if the original path fpath has no suffix, then new_suffix will be
directly applied at the end of the fpath string, joint by separator <cite>.</cite></p>
<p>In : change_suffix(‘xxx/ddd/www.x’,’aa’)
Out: ‘xxx/ddd/www.aa’</p>
<p>In : change_suffix(‘xxx/ddd/www’,’aa’)
Out: ‘xxx/ddd/www.aa’</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.cpu_memory">
<code class="descclassname">torchfun.utils.</code><code class="descname">cpu_memory</code><span class="sig-paren">(</span><em>print_on_screen=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.cpu_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>total CPU memory usage of current python session.
returns: (RSS,VMS) In bytes!</p>
<blockquote>
<div><p>RSS is resident set size, 
VMS is virtual memory size.</p>
</div></blockquote>
<dl class="simple">
<dt>Notice: </dt><dd><p>return values are in bytes.
printed values are in MBs.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.force_exist">
<code class="descclassname">torchfun.utils.</code><code class="descname">force_exist</code><span class="sig-paren">(</span><em>dirname</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.force_exist" title="Permalink to this definition">¶</a></dt>
<dd><p>force a directory to exist.
force_exist can automatically create directory with any depth.
Arguements:</p>
<blockquote>
<div><p>dirname: path of the desired directory
verbose: print every directory creation. default True.</p>
</div></blockquote>
<dl class="simple">
<dt>Usage:</dt><dd><p>force_exist(‘a/b/c/d/e/f’)
force_exist(‘a/b/c/d/e/f’,verbose=False)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.omini_open">
<code class="descclassname">torchfun.utils.</code><code class="descname">omini_open</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.omini_open" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens everything using system default viwer.</p>
<p>This function can call system GUI to open folders,images,files,videos…</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.print_verbose">
<code class="descclassname">torchfun.utils.</code><code class="descname">print_verbose</code><span class="sig-paren">(</span><em>*args</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.print_verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>programmable print function. with an option to control
if the inputs are really printed.
used to control verbose levels of a function.
verbose: default True.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.printf">
<code class="descclassname">torchfun.utils.</code><code class="descname">printf</code><span class="sig-paren">(</span><em>format_str</em>, <em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.printf" title="Permalink to this definition">¶</a></dt>
<dd><p>works like C printf.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.record_experiment">
<code class="descclassname">torchfun.utils.</code><code class="descname">record_experiment</code><span class="sig-paren">(</span><em>exp_dir='not-specified'</em>, <em>record_top_dir='records'</em>, <em>logfilename='record.txt'</em>, <em>comment=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.record_experiment" title="Permalink to this definition">¶</a></dt>
<dd><p>Arguments:
* exp_dir : directory of this experiment output files,</p>
<blockquote>
<div><p>recorded so that it would be easier for you to find the outcome files 
of this experiment later.</p>
</div></blockquote>
<ul class="simple">
<li><dl class="simple">
<dt>record_top_dir<span class="classifier">create a dir to save all kinds of record logs. default(records). </span></dt><dd><p>set to empty string(‘’) to save all record in the current dir.</p>
</dd>
</dl>
</li>
<li><p>logfilename : filename of this log, usually <cite>train_record</cite> <cite>evaluation_record</cite> etc.</p></li>
<li><p>comment :string comment added to log paragraph. default is empty string.</p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.reset_timer">
<code class="descclassname">torchfun.utils.</code><code class="descname">reset_timer</code><span class="sig-paren">(</span><em>named_time_table={}, anonymous_time=[None]</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.reset_timer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.utils.safe_open">
<code class="descclassname">torchfun.utils.</code><code class="descname">safe_open</code><span class="sig-paren">(</span><em>*args</em>, <em>encoding=None</em>, <em>return_encoding=False</em>, <em>verbose=True</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.safe_open" title="Permalink to this definition">¶</a></dt>
<dd><p>automatically determine the encoding of the file.
so that there will not be so many stupid encoding errors occuring during coding.</p>
<dl class="simple">
<dt>Note: the file needs to be fully loaded into RAM to examine the encodings. </dt><dd><p>OOM(Out Of Memory) exception may be raised when encountering large text files.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.sort_args">
<code class="descclassname">torchfun.utils.</code><code class="descname">sort_args</code><span class="sig-paren">(</span><em>args_or_types</em>, <em>types_or_args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.sort_args" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a very interesting function.
It is used to support __arbitrary-<a href="#id31"><span class="problematic" id="id32">arguments-ordering__</span></a> in TorchFun.</p>
<dl class="simple">
<dt>Input:</dt><dd><p>The function takes a list of types, and a list of arguments.</p>
</dd>
<dt>Returns:</dt><dd><p>a list of arguments, with the same order as the types-list.</p>
</dd>
</dl>
<p>Of course, <cite>sort_args</cite> supports arbitrary-arguments-ordering by itself.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.utils.time">
<code class="descclassname">torchfun.utils.</code><code class="descname">time</code><span class="sig-paren">(</span><em>message=None, name=None, named_time_table={}, anonymous_time=[None]</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.utils.time" title="Permalink to this definition">¶</a></dt>
<dd><p>show running time
dict-key locating costs 1e-6 second = 1 micro-second
context switching costs 5e-5 second.</p>
<dl class="simple">
<dt>Usage 1:</dt><dd><p>tf.time()
…
elapsed = tf.time()</p>
</dd>
<dt>Usage 2:</dt><dd><p>tf.time()
…
tf.time(‘elapsed:’)
out: elapsed: 0.02 sec</p>
</dd>
<dt>Usage 3:</dt><dd><p>tf.time(name=’clock1’)
…
tf.time(‘elapsed’,name=’clock1’)
out: elapsed 0.02 sec</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torchfun">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-torchfun" title="Permalink to this headline">¶</a></h2>
<p><cite>TorchFun</cite> project was initiated long ago and was published in 2018-6-13.</p>
<p>TorchFun is motivated by the one author loves, who sometimes encounter inconvenience using PyTorch at her work. The author published the project as a python package <cite>TorchFun</cite> on PyPi, so that his beloved could access it whenever and wheresoever needed.</p>
<p>The purposed of TorchFun is to provide functions which are important and convenient, but absent in PyTorch, like some layers and visualization utils.</p>
<p>This project has been undergoing in secret so that the author could credit TorchFun to his beloved as a little supprise of help, one day when this project is well-enriched or gets famous.</p>
<p>Interestingly, The original project name given by the author, is <cite>Torchure</cite>. That is because he was always multi-tasking trivial affairs in school and got scorched, and when he was learning this new framework in hope to help his beloved, he found plenty of issues/missing-functionalities. He felt that this was totally a torture. So, this project was named “Torchure” to satirize the lame PyTorch (you can still found Torchure in PyPi). And, the author hoped, by developing Torchure, his beloved could feel ease even when encountering the crappy-parts of PyTorch.</p>
<p>This history-of-project was appended recently, because his adorable little beloved wants a supprise immediately, or she will keep rolling on the floor.</p>
<p>To c71b05bf46d8772e4488335085a2e7fd.</p>
<dl class="function">
<dt id="torchfun.argparse_list_type">
<code class="descclassname">torchfun.</code><code class="descname">argparse_list_type</code><span class="sig-paren">(</span><em>element_type=&lt;class 'int'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.argparse_list_type" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=?),
in order to parse lists like:
–gpu-ids=1,2,3,4</p>
<dl class="simple">
<dt>usage:</dt><dd><p>floatlist = argparse_list_type(float)
parser.add_argument(‘–cuda-devs’,type=flaotlist,default=[1,2])</p>
</dd>
<dt>args:</dt><dd><dl class="simple">
<dt>element_type: type of each element in the list,</dt><dd><p>eache element will be cat to this type.</p>
</dd>
</dl>
</dd>
</dl>
<p>Notice: Instances must be used, instead of this class itself.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.list_of_int">
<code class="descclassname">torchfun.</code><code class="descname">list_of_int</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.list_of_int" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.list_of_float">
<code class="descclassname">torchfun.</code><code class="descname">list_of_float</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.list_of_float" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.argparse_bool_type">
<code class="descclassname">torchfun.</code><code class="descname">argparse_bool_type</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.argparse_bool_type" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=torchfun.bool),
in order to parse bool switch values like:
false False true True 0 1</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.BufferingFormatter">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">BufferingFormatter</code><span class="sig-paren">(</span><em>linefmt=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.BufferingFormatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A formatter suitable for formatting a number of records.</p>
<dl class="method">
<dt id="torchfun.BufferingFormatter.format">
<code class="descname">format</code><span class="sig-paren">(</span><em>records</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.BufferingFormatter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Format the specified records and return the result as a string.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.BufferingFormatter.formatFooter">
<code class="descname">formatFooter</code><span class="sig-paren">(</span><em>records</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.BufferingFormatter.formatFooter" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the footer string for the specified records.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.BufferingFormatter.formatHeader">
<code class="descname">formatHeader</code><span class="sig-paren">(</span><em>records</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.BufferingFormatter.formatHeader" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the header string for the specified records.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.FileHandler">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">FileHandler</code><span class="sig-paren">(</span><em>filename</em>, <em>mode='a'</em>, <em>encoding=None</em>, <em>delay=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.FileHandler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">logging.StreamHandler</span></code></p>
<p>A handler class which writes formatted logging records to disk files.</p>
<dl class="method">
<dt id="torchfun.FileHandler.close">
<code class="descname">close</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.FileHandler.close" title="Permalink to this definition">¶</a></dt>
<dd><p>Closes the stream.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.FileHandler.emit">
<code class="descname">emit</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.FileHandler.emit" title="Permalink to this definition">¶</a></dt>
<dd><p>Emit a record.</p>
<p>If the stream was not opened because ‘delay’ was specified in the
constructor, open it before calling the superclass’s emit.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Filter">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Filter</code><span class="sig-paren">(</span><em>name=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Filter instances are used to perform arbitrary filtering of LogRecords.</p>
<p>Loggers and Handlers can optionally use Filter instances to filter
records as desired. The base filter class only allows events which are
below a certain point in the logger hierarchy. For example, a filter
initialized with “A.B” will allow events logged by loggers “A.B”,
“A.B.C”, “A.B.C.D”, “A.B.D” etc. but not “A.BB”, “B.A.B” etc. If
initialized with the empty string, all events are passed.</p>
<dl class="method">
<dt id="torchfun.Filter.filter">
<code class="descname">filter</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Filter.filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Determine if the specified record is to be logged.</p>
<p>Is the specified record to be logged? Returns 0 for no, nonzero for
yes. If deemed appropriate, the record may be modified in-place.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Formatter">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Formatter</code><span class="sig-paren">(</span><em>fmt=None</em>, <em>datefmt=None</em>, <em>style='%'</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Formatter instances are used to convert a LogRecord to text.</p>
<p>Formatters need to know how a LogRecord is constructed. They are
responsible for converting a LogRecord to (usually) a string which can
be interpreted by either a human or an external system. The base Formatter
allows a formatting string to be specified. If none is supplied, the
default value of “%s(message)” is used.</p>
<p>The Formatter can be initialized with a format string which makes use of
knowledge of the LogRecord attributes - e.g. the default value mentioned
above makes use of the fact that the user’s message and arguments are pre-
formatted into a LogRecord’s message attribute. Currently, the useful
attributes in a LogRecord are described by:</p>
<p>%(name)s            Name of the logger (logging channel)
%(levelno)s         Numeric logging level for the message (DEBUG, INFO,</p>
<blockquote>
<div><p>WARNING, ERROR, CRITICAL)</p>
</div></blockquote>
<dl class="simple">
<dt>%(levelname)s       Text logging level for the message (“DEBUG”, “INFO”,</dt><dd><p>“WARNING”, “ERROR”, “CRITICAL”)</p>
</dd>
<dt>%(pathname)s        Full pathname of the source file where the logging</dt><dd><p>call was issued (if available)</p>
</dd>
</dl>
<p>%(filename)s        Filename portion of pathname
%(module)s          Module (name portion of filename)
%(lineno)d          Source line number where the logging call was issued</p>
<blockquote>
<div><p>(if available)</p>
</div></blockquote>
<p>%(funcName)s        Function name
%(created)f         Time when the LogRecord was created (time.time()</p>
<blockquote>
<div><p>return value)</p>
</div></blockquote>
<p>%(asctime)s         Textual time when the LogRecord was created
%(msecs)d           Millisecond portion of the creation time
%(relativeCreated)d Time in milliseconds when the LogRecord was created,</p>
<blockquote>
<div><p>relative to the time the logging module was loaded
(typically at application startup time)</p>
</div></blockquote>
<p>%(thread)d          Thread ID (if available)
%(threadName)s      Thread name (if available)
%(process)d         Process ID (if available)
%(message)s         The result of record.getMessage(), computed just as</p>
<blockquote>
<div><p>the record is emitted</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.Formatter.converter">
<code class="descname">converter</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.converter" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>localtime([seconds]) -&gt; (tm_year,tm_mon,tm_mday,tm_hour,tm_min,</dt><dd><p>tm_sec,tm_wday,tm_yday,tm_isdst)</p>
</dd>
</dl>
<p>Convert seconds since the Epoch to a time tuple expressing local time.
When ‘seconds’ is not passed in, convert the current time instead.</p>
</dd></dl>

<dl class="attribute">
<dt id="torchfun.Formatter.default_msec_format">
<code class="descname">default_msec_format</code><em class="property"> = '%s,%03d'</em><a class="headerlink" href="#torchfun.Formatter.default_msec_format" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torchfun.Formatter.default_time_format">
<code class="descname">default_time_format</code><em class="property"> = '%Y-%m-%d %H:%M:%S'</em><a class="headerlink" href="#torchfun.Formatter.default_time_format" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Formatter.format">
<code class="descname">format</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Format the specified record as text.</p>
<p>The record’s attribute dictionary is used as the operand to a
string formatting operation which yields the returned string.
Before formatting the dictionary, a couple of preparatory steps
are carried out. The message attribute of the record is computed
using LogRecord.getMessage(). If the formatting string uses the
time (as determined by a call to usesTime(), formatTime() is
called to format the event time. If there is exception information,
it is formatted using formatException() and appended to the message.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Formatter.formatException">
<code class="descname">formatException</code><span class="sig-paren">(</span><em>ei</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.formatException" title="Permalink to this definition">¶</a></dt>
<dd><p>Format and return the specified exception information as a string.</p>
<p>This default implementation just uses
traceback.print_exception()</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Formatter.formatMessage">
<code class="descname">formatMessage</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.formatMessage" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Formatter.formatStack">
<code class="descname">formatStack</code><span class="sig-paren">(</span><em>stack_info</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.formatStack" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is provided as an extension point for specialized
formatting of stack information.</p>
<p>The input data is a string as returned from a call to
<code class="xref py py-func docutils literal notranslate"><span class="pre">traceback.print_stack()</span></code>, but with the last trailing newline
removed.</p>
<p>The base implementation just returns the value passed in.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Formatter.formatTime">
<code class="descname">formatTime</code><span class="sig-paren">(</span><em>record</em>, <em>datefmt=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.formatTime" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the creation time of the specified LogRecord as formatted text.</p>
<p>This method should be called from format() by a formatter which
wants to make use of a formatted time. This method can be overridden
in formatters to provide for any specific requirement, but the
basic behaviour is as follows: if datefmt (a string) is specified,
it is used with time.strftime() to format the creation time of the
record. Otherwise, the ISO8601 format is used. The resulting
string is returned. This function uses a user-configurable function
to convert the creation time to a tuple. By default, time.localtime()
is used; to change this for a particular formatter instance, set the
‘converter’ attribute to a function with the same signature as
time.localtime() or time.gmtime(). To change it for all formatters,
for example if you want all logging times to be shown in GMT,
set the ‘converter’ attribute in the Formatter class.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Formatter.usesTime">
<code class="descname">usesTime</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Formatter.usesTime" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if the format uses the creation time of the record.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Handler">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Handler</code><span class="sig-paren">(</span><em>level=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">logging.Filterer</span></code></p>
<p>Handler instances dispatch logging events to specific destinations.</p>
<p>The base handler class. Acts as a placeholder which defines the Handler
interface. Handlers can optionally use Formatter instances to format
records as desired. By default, no formatter is specified; in this case,
the ‘raw’ message as determined by record.message is logged.</p>
<dl class="method">
<dt id="torchfun.Handler.acquire">
<code class="descname">acquire</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.acquire" title="Permalink to this definition">¶</a></dt>
<dd><p>Acquire the I/O thread lock.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.close">
<code class="descname">close</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.close" title="Permalink to this definition">¶</a></dt>
<dd><p>Tidy up any resources used by the handler.</p>
<p>This version removes the handler from an internal map of handlers,
_handlers, which is used for handler lookup by name. Subclasses
should ensure that this gets called from overridden close()
methods.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.createLock">
<code class="descname">createLock</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.createLock" title="Permalink to this definition">¶</a></dt>
<dd><p>Acquire a thread lock for serializing access to the underlying I/O.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.emit">
<code class="descname">emit</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.emit" title="Permalink to this definition">¶</a></dt>
<dd><p>Do whatever it takes to actually log the specified logging record.</p>
<p>This version is intended to be implemented by subclasses and so
raises a NotImplementedError.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.flush">
<code class="descname">flush</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.flush" title="Permalink to this definition">¶</a></dt>
<dd><p>Ensure all logging output has been flushed.</p>
<p>This version does nothing and is intended to be implemented by
subclasses.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.format">
<code class="descname">format</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Format the specified record.</p>
<p>If a formatter is set, use it. Otherwise, use the default formatter
for the module.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.get_name">
<code class="descname">get_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.get_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Handler.handle">
<code class="descname">handle</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Conditionally emit the specified logging record.</p>
<p>Emission depends on filters which may have been added to the handler.
Wrap the actual emission of the record with acquisition/release of
the I/O thread lock. Returns whether the filter passed the record for
emission.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.handleError">
<code class="descname">handleError</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.handleError" title="Permalink to this definition">¶</a></dt>
<dd><p>Handle errors which occur during an emit() call.</p>
<p>This method should be called from handlers when an exception is
encountered during an emit() call. If raiseExceptions is false,
exceptions get silently ignored. This is what is mostly wanted
for a logging system - most users will not care about errors in
the logging system, they are more interested in application errors.
You could, however, replace this with a custom handler if you wish.
The record which was being processed is passed in to this method.</p>
</dd></dl>

<dl class="attribute">
<dt id="torchfun.Handler.name">
<code class="descname">name</code><a class="headerlink" href="#torchfun.Handler.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Handler.release">
<code class="descname">release</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.release" title="Permalink to this definition">¶</a></dt>
<dd><p>Release the I/O thread lock.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.setFormatter">
<code class="descname">setFormatter</code><span class="sig-paren">(</span><em>fmt</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.setFormatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the formatter for this handler.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.setLevel">
<code class="descname">setLevel</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.setLevel" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the logging level of this handler.  level must be an int or a str.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Handler.set_name">
<code class="descname">set_name</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Handler.set_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.LogRecord">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">LogRecord</code><span class="sig-paren">(</span><em>name</em>, <em>level</em>, <em>pathname</em>, <em>lineno</em>, <em>msg</em>, <em>args</em>, <em>exc_info</em>, <em>func=None</em>, <em>sinfo=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LogRecord" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A LogRecord instance represents an event being logged.</p>
<p>LogRecord instances are created every time something is logged. They
contain all the information pertinent to the event being logged. The
main information passed in is in msg and args, which are combined
using str(msg) % args to create the message field of the record. The
record also includes information such as when the record was created,
the source line where the logging call was made, and any exception
information to be logged.</p>
<dl class="method">
<dt id="torchfun.LogRecord.getMessage">
<code class="descname">getMessage</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LogRecord.getMessage" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the message for this LogRecord.</p>
<p>Return the message for this LogRecord after merging any user-supplied
arguments with the message.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Logger">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Logger</code><span class="sig-paren">(</span><em>name</em>, <em>level=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">logging.Filterer</span></code></p>
<p>Instances of the Logger class represent a single logging channel. A
“logging channel” indicates an area of an application. Exactly how an
“area” is defined is up to the application developer. Since an
application can have any number of areas, logging channels are identified
by a unique string. Application areas can be nested (e.g. an area
of “input processing” might include sub-areas “read CSV files”, “read
XLS files” and “read Gnumeric files”). To cater for this natural nesting,
channel names are organized into a namespace hierarchy where levels are
separated by periods, much like the Java or Python package namespace. So
in the instance given above, channel names might be “input” for the upper
level, and “input.csv”, “input.xls” and “input.gnu” for the sub-levels.
There is no arbitrary limit to the depth of nesting.</p>
<dl class="method">
<dt id="torchfun.Logger.addHandler">
<code class="descname">addHandler</code><span class="sig-paren">(</span><em>hdlr</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.addHandler" title="Permalink to this definition">¶</a></dt>
<dd><p>Add the specified handler to this logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.callHandlers">
<code class="descname">callHandlers</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.callHandlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Pass a record to all relevant handlers.</p>
<p>Loop through all handlers for this logger and its parents in the
logger hierarchy. If no handler was found, output a one-off error
message to sys.stderr. Stop searching up the hierarchy whenever a
logger with the “propagate” attribute set to zero is found - that
will be the last logger whose handlers are called.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.critical">
<code class="descname">critical</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.critical" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with severity ‘CRITICAL’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.critical(“Houston, we have a %s”, “major disaster”, exc_info=1)</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.debug">
<code class="descname">debug</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.debug" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with severity ‘DEBUG’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.debug(“Houston, we have a %s”, “thorny problem”, exc_info=1)</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.error">
<code class="descname">error</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.error" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with severity ‘ERROR’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.error(“Houston, we have a %s”, “major problem”, exc_info=1)</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.exception">
<code class="descname">exception</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>exc_info=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience method for logging an ERROR with exception information.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.fatal">
<code class="descname">fatal</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.fatal" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with severity ‘CRITICAL’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.critical(“Houston, we have a %s”, “major disaster”, exc_info=1)</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.findCaller">
<code class="descname">findCaller</code><span class="sig-paren">(</span><em>stack_info=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.findCaller" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the stack frame of the caller so that we can note the source
file name, line number and function name.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.getChild">
<code class="descname">getChild</code><span class="sig-paren">(</span><em>suffix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.getChild" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a logger which is a descendant to this one.</p>
<p>This is a convenience method, such that</p>
<p>logging.getLogger(‘abc’).getChild(‘def.ghi’)</p>
<p>is the same as</p>
<p>logging.getLogger(‘abc.def.ghi’)</p>
<p>It’s useful, for example, when the parent logger is named using
__name__ rather than a literal string.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.getEffectiveLevel">
<code class="descname">getEffectiveLevel</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.getEffectiveLevel" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the effective level for this logger.</p>
<p>Loop through this logger and its parents in the logger hierarchy,
looking for a non-zero logging level. Return the first one found.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.handle">
<code class="descname">handle</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the handlers for the specified record.</p>
<p>This method is used for unpickled records received from a socket, as
well as those created locally. Logger-level filtering is applied.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.hasHandlers">
<code class="descname">hasHandlers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.hasHandlers" title="Permalink to this definition">¶</a></dt>
<dd><p>See if this logger has any handlers configured.</p>
<p>Loop through all handlers for this logger and its parents in the
logger hierarchy. Return True if a handler was found, else False.
Stop searching up the hierarchy whenever a logger with the “propagate”
attribute set to zero is found - that will be the last logger which
is checked for the existence of handlers.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.info">
<code class="descname">info</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.info" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with severity ‘INFO’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.info(“Houston, we have a %s”, “interesting problem”, exc_info=1)</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.isEnabledFor">
<code class="descname">isEnabledFor</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.isEnabledFor" title="Permalink to this definition">¶</a></dt>
<dd><p>Is this logger enabled for level ‘level’?</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.log">
<code class="descname">log</code><span class="sig-paren">(</span><em>level</em>, <em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with the integer severity ‘level’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.log(level, “We have a %s”, “mysterious problem”, exc_info=1)</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.makeRecord">
<code class="descname">makeRecord</code><span class="sig-paren">(</span><em>name</em>, <em>level</em>, <em>fn</em>, <em>lno</em>, <em>msg</em>, <em>args</em>, <em>exc_info</em>, <em>func=None</em>, <em>extra=None</em>, <em>sinfo=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.makeRecord" title="Permalink to this definition">¶</a></dt>
<dd><p>A factory method which can be overridden in subclasses to create
specialized LogRecords.</p>
</dd></dl>

<dl class="attribute">
<dt id="torchfun.Logger.manager">
<code class="descname">manager</code><em class="property"> = &lt;logging.Manager object&gt;</em><a class="headerlink" href="#torchfun.Logger.manager" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Logger.removeHandler">
<code class="descname">removeHandler</code><span class="sig-paren">(</span><em>hdlr</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.removeHandler" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the specified handler from this logger.</p>
</dd></dl>

<dl class="attribute">
<dt id="torchfun.Logger.root">
<code class="descname">root</code><em class="property"> = &lt;RootLogger root (WARNING)&gt;</em><a class="headerlink" href="#torchfun.Logger.root" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Logger.setLevel">
<code class="descname">setLevel</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.setLevel" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the logging level of this logger.  level must be an int or a str.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Logger.warn">
<code class="descname">warn</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.warn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Logger.warning">
<code class="descname">warning</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Logger.warning" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with severity ‘WARNING’.</p>
<p>To pass exception information, use the keyword argument exc_info with
a true value, e.g.</p>
<p>logger.warning(“Houston, we have a %s”, “bit of a problem”, exc_info=1)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.LoggerAdapter">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">LoggerAdapter</code><span class="sig-paren">(</span><em>logger</em>, <em>extra</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An adapter for loggers which makes it easier to specify contextual
information in logging output.</p>
<dl class="method">
<dt id="torchfun.LoggerAdapter.critical">
<code class="descname">critical</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.critical" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate a critical call to the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.debug">
<code class="descname">debug</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.debug" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate a debug call to the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.error">
<code class="descname">error</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.error" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate an error call to the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.exception">
<code class="descname">exception</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>exc_info=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate an exception call to the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.getEffectiveLevel">
<code class="descname">getEffectiveLevel</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.getEffectiveLevel" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the effective level for the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.hasHandlers">
<code class="descname">hasHandlers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.hasHandlers" title="Permalink to this definition">¶</a></dt>
<dd><p>See if the underlying logger has any handlers.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.info">
<code class="descname">info</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.info" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate an info call to the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.isEnabledFor">
<code class="descname">isEnabledFor</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.isEnabledFor" title="Permalink to this definition">¶</a></dt>
<dd><p>Is this logger enabled for level ‘level’?</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.log">
<code class="descname">log</code><span class="sig-paren">(</span><em>level</em>, <em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate a log call to the underlying logger, after adding
contextual information from this adapter instance.</p>
</dd></dl>

<dl class="attribute">
<dt id="torchfun.LoggerAdapter.manager">
<code class="descname">manager</code><a class="headerlink" href="#torchfun.LoggerAdapter.manager" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torchfun.LoggerAdapter.name">
<code class="descname">name</code><a class="headerlink" href="#torchfun.LoggerAdapter.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>msg</em>, <em>kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Process the logging message and keyword arguments passed in to
a logging call to insert contextual information. You can either
manipulate the message itself, the keyword args or both. Return
the message and kwargs modified (or not) to suit your needs.</p>
<p>Normally, you’ll only need to override this one method in a
LoggerAdapter subclass for your specific needs.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.setLevel">
<code class="descname">setLevel</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.setLevel" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the specified level on the underlying logger.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.warn">
<code class="descname">warn</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.warn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.LoggerAdapter.warning">
<code class="descname">warning</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LoggerAdapter.warning" title="Permalink to this definition">¶</a></dt>
<dd><p>Delegate a warning call to the underlying logger.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.NullHandler">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">NullHandler</code><span class="sig-paren">(</span><em>level=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NullHandler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">logging.Handler</span></code></p>
<p>This handler does nothing. It’s intended to be used to avoid the
“No handlers could be found for logger XXX” one-off warning. This is
important for library code, which may contain code to log events. If a user
of the library does not configure logging, the one-off warning might be
produced; to avoid this, the library developer simply needs to instantiate
a NullHandler and add it to the top-level logger of the library module or
package.</p>
<dl class="method">
<dt id="torchfun.NullHandler.createLock">
<code class="descname">createLock</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NullHandler.createLock" title="Permalink to this definition">¶</a></dt>
<dd><p>Acquire a thread lock for serializing access to the underlying I/O.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.NullHandler.emit">
<code class="descname">emit</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NullHandler.emit" title="Permalink to this definition">¶</a></dt>
<dd><p>Stub.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.NullHandler.handle">
<code class="descname">handle</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NullHandler.handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Stub.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.StreamHandler">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">StreamHandler</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.StreamHandler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">logging.Handler</span></code></p>
<p>A handler class which writes logging records, appropriately formatted,
to a stream. Note that this class does not close the stream, as
sys.stdout or sys.stderr may be used.</p>
<dl class="method">
<dt id="torchfun.StreamHandler.emit">
<code class="descname">emit</code><span class="sig-paren">(</span><em>record</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.StreamHandler.emit" title="Permalink to this definition">¶</a></dt>
<dd><p>Emit a record.</p>
<p>If a formatter is specified, it is used to format the record.
The record is then written to the stream with a trailing newline.  If
exception information is present, it is formatted using
traceback.print_exception and appended to the stream.  If the stream
has an ‘encoding’ attribute, it is used to determine how to do the
output to the stream.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.StreamHandler.flush">
<code class="descname">flush</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.StreamHandler.flush" title="Permalink to this definition">¶</a></dt>
<dd><p>Flushes the stream.</p>
</dd></dl>

<dl class="attribute">
<dt id="torchfun.StreamHandler.terminator">
<code class="descname">terminator</code><em class="property"> = '\n'</em><a class="headerlink" href="#torchfun.StreamHandler.terminator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.addLevelName">
<code class="descclassname">torchfun.</code><code class="descname">addLevelName</code><span class="sig-paren">(</span><em>level</em>, <em>levelName</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.addLevelName" title="Permalink to this definition">¶</a></dt>
<dd><p>Associate ‘levelName’ with ‘level’.</p>
<p>This is used when converting levels to text during message formatting.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.basicConfig">
<code class="descclassname">torchfun.</code><code class="descname">basicConfig</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.basicConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Do basic configuration for the logging system.</p>
<p>This function does nothing if the root logger already has handlers
configured. It is a convenience method intended for use by simple scripts
to do one-shot configuration of the logging package.</p>
<p>The default behaviour is to create a StreamHandler which writes to
sys.stderr, set a formatter using the BASIC_FORMAT format string, and
add the handler to the root logger.</p>
<p>A number of optional keyword arguments may be specified, which can alter
the default behaviour.</p>
<dl class="simple">
<dt>filename  Specifies that a FileHandler be created, using the specified</dt><dd><p>filename, rather than a StreamHandler.</p>
</dd>
<dt>filemode  Specifies the mode to open the file, if filename is specified</dt><dd><p>(if filemode is unspecified, it defaults to ‘a’).</p>
</dd>
</dl>
<p>format    Use the specified format string for the handler.
datefmt   Use the specified date/time format.
style     If a format string is specified, use this to specify the</p>
<blockquote>
<div><p>type of format string (possible values ‘%’, ‘{‘, ‘$’, for
%-formatting, <code class="xref py py-meth docutils literal notranslate"><span class="pre">str.format()</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">string.Template</span></code>
- defaults to ‘%’).</p>
</div></blockquote>
<p>level     Set the root logger level to the specified level.
stream    Use the specified stream to initialize the StreamHandler. Note</p>
<blockquote>
<div><p>that this argument is incompatible with ‘filename’ - if both
are present, ‘stream’ is ignored.</p>
</div></blockquote>
<dl class="simple">
<dt>handlers  If specified, this should be an iterable of already created</dt><dd><p>handlers, which will be added to the root handler. Any handler
in the list which does not have a formatter assigned will be
assigned the formatter created in this function.</p>
</dd>
</dl>
<p>Note that you could specify a stream created using open(filename, mode)
rather than passing the filename and mode in. However, it should be
remembered that StreamHandler does not close its stream (since it may be
using sys.stdout or sys.stderr), whereas FileHandler closes its stream
when the handler is closed.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 3.2: </span>Added the <code class="docutils literal notranslate"><span class="pre">style</span></code> parameter.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 3.3: </span>Added the <code class="docutils literal notranslate"><span class="pre">handlers</span></code> parameter. A <code class="docutils literal notranslate"><span class="pre">ValueError</span></code> is now thrown for
incompatible arguments (e.g. <code class="docutils literal notranslate"><span class="pre">handlers</span></code> specified together with
<code class="docutils literal notranslate"><span class="pre">filename</span></code>/<code class="docutils literal notranslate"><span class="pre">filemode</span></code>, or <code class="docutils literal notranslate"><span class="pre">filename</span></code>/<code class="docutils literal notranslate"><span class="pre">filemode</span></code> specified
together with <code class="docutils literal notranslate"><span class="pre">stream</span></code>, or <code class="docutils literal notranslate"><span class="pre">handlers</span></code> specified together with
<code class="docutils literal notranslate"><span class="pre">stream</span></code>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torchfun.captureWarnings">
<code class="descclassname">torchfun.</code><code class="descname">captureWarnings</code><span class="sig-paren">(</span><em>capture</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.captureWarnings" title="Permalink to this definition">¶</a></dt>
<dd><p>If capture is true, redirect all warnings to the logging package.
If capture is False, ensure that warnings are not redirected to logging
but to their original destinations.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.critical">
<code class="descclassname">torchfun.</code><code class="descname">critical</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.critical" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘CRITICAL’ on the root logger. If the logger
has no handlers, call basicConfig() to add a console handler with a
pre-defined format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.debug">
<code class="descclassname">torchfun.</code><code class="descname">debug</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.debug" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘DEBUG’ on the root logger. If the logger has
no handlers, call basicConfig() to add a console handler with a pre-defined
format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.disable">
<code class="descclassname">torchfun.</code><code class="descname">disable</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.disable" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable all logging calls of severity ‘level’ and below.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.error">
<code class="descclassname">torchfun.</code><code class="descname">error</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.error" title="Permalink to this definition">¶</a></dt>
<dd><p>throw error and exit the program</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.exception">
<code class="descclassname">torchfun.</code><code class="descname">exception</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>exc_info=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘ERROR’ on the root logger, with exception
information. If the logger has no handlers, basicConfig() is called to add
a console handler with a pre-defined format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.fatal">
<code class="descclassname">torchfun.</code><code class="descname">fatal</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.fatal" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘CRITICAL’ on the root logger. If the logger
has no handlers, call basicConfig() to add a console handler with a
pre-defined format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.getLevelName">
<code class="descclassname">torchfun.</code><code class="descname">getLevelName</code><span class="sig-paren">(</span><em>level</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.getLevelName" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the textual representation of logging level ‘level’.</p>
<p>If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,
INFO, DEBUG) then you get the corresponding string. If you have
associated levels with names using addLevelName then the name you have
associated with ‘level’ is returned.</p>
<p>If a numeric value corresponding to one of the defined levels is passed
in, the corresponding string representation is returned.</p>
<p>Otherwise, the string “Level %s” % level is returned.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.getLogger">
<code class="descclassname">torchfun.</code><code class="descname">getLogger</code><span class="sig-paren">(</span><em>name=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.getLogger" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a logger with the specified name, creating it if necessary.</p>
<p>If no name is specified, return the root logger.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.getLoggerClass">
<code class="descclassname">torchfun.</code><code class="descname">getLoggerClass</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.getLoggerClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the class to be used when instantiating a logger.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.info">
<code class="descclassname">torchfun.</code><code class="descname">info</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.info" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘INFO’ on the root logger. If the logger has
no handlers, call basicConfig() to add a console handler with a pre-defined
format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.log">
<code class="descclassname">torchfun.</code><code class="descname">log</code><span class="sig-paren">(</span><em>level</em>, <em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log ‘msg % args’ with the integer severity ‘level’ on the root logger. If
the logger has no handlers, call basicConfig() to add a console handler
with a pre-defined format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.makeLogRecord">
<code class="descclassname">torchfun.</code><code class="descname">makeLogRecord</code><span class="sig-paren">(</span><em>dict</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.makeLogRecord" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a LogRecord whose attributes are defined by the specified dictionary,
This function is useful for converting a logging event received over
a socket connection (which is sent as a dictionary) into a LogRecord
instance.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.setLoggerClass">
<code class="descclassname">torchfun.</code><code class="descname">setLoggerClass</code><span class="sig-paren">(</span><em>klass</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.setLoggerClass" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the class to be used when instantiating a logger. The class should
define __init__() such that only a name argument is required, and the
__init__() should call Logger.__init__()</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.shutdown">
<code class="descclassname">torchfun.</code><code class="descname">shutdown</code><span class="sig-paren">(</span><em>handlerList=[&lt;weakref at 0x00000248AD7A8EA8; to '_StderrHandler'&gt;, &lt;weakref at 0x00000248AEF22EA8; to 'NewLineStreamHandler'&gt;, &lt;weakref at 0x00000248AEF22F48; to 'WarningStreamHandler'&gt;, &lt;weakref at 0x00000248AEF3D048; to 'StreamHandler'&gt;, &lt;weakref at 0x00000248AF4C7408; to 'NullHandler'&gt;, &lt;weakref at 0x00000248AF6A5F98; to 'NullHandler'&gt;, &lt;weakref at 0x00000248AFD2A908; to 'MemoryHandler'&gt;]</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.shutdown" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform any cleanup actions in the logging system (e.g. flushing
buffers).</p>
<p>Should be called at application exit.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.warn">
<code class="descclassname">torchfun.</code><code class="descname">warn</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.warn" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘WARNING’ on the root logger. If the logger has
no handlers, call basicConfig() to add a console handler with a pre-defined
format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.warning">
<code class="descclassname">torchfun.</code><code class="descname">warning</code><span class="sig-paren">(</span><em>msg</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.warning" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a message with severity ‘WARNING’ on the root logger. If the logger has
no handlers, call basicConfig() to add a console handler with a pre-defined
format.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.getLogRecordFactory">
<code class="descclassname">torchfun.</code><code class="descname">getLogRecordFactory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.getLogRecordFactory" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the factory to be used when instantiating a log record.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.setLogRecordFactory">
<code class="descclassname">torchfun.</code><code class="descname">setLogRecordFactory</code><span class="sig-paren">(</span><em>factory</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.setLogRecordFactory" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the factory to be used when instantiating a log record.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>factory</strong> – A callable which will be called to instantiate</p>
</dd>
</dl>
<p>a log record.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.wait">
<code class="descclassname">torchfun.</code><code class="descname">wait</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>wait and show a message, until a key is stroked.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.input_or">
<code class="descclassname">torchfun.</code><code class="descname">input_or</code><span class="sig-paren">(</span><em>prompt</em>, <em>default='y'</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.input_or" title="Permalink to this definition">¶</a></dt>
<dd><p>get input from command-line,
if  the input is ommited by an enter key, then the default values is returned.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.printf">
<code class="descclassname">torchfun.</code><code class="descname">printf</code><span class="sig-paren">(</span><em>format_str</em>, <em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.printf" title="Permalink to this definition">¶</a></dt>
<dd><p>works like C printf.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.print_verbose">
<code class="descclassname">torchfun.</code><code class="descname">print_verbose</code><span class="sig-paren">(</span><em>*args</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.print_verbose" title="Permalink to this definition">¶</a></dt>
<dd><p>programmable print function. with an option to control
if the inputs are really printed.
used to control verbose levels of a function.
verbose: default True.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.safe_open">
<code class="descclassname">torchfun.</code><code class="descname">safe_open</code><span class="sig-paren">(</span><em>*args</em>, <em>encoding=None</em>, <em>return_encoding=False</em>, <em>verbose=True</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.safe_open" title="Permalink to this definition">¶</a></dt>
<dd><p>automatically determine the encoding of the file.
so that there will not be so many stupid encoding errors occuring during coding.</p>
<dl class="simple">
<dt>Note: the file needs to be fully loaded into RAM to examine the encodings. </dt><dd><p>OOM(Out Of Memory) exception may be raised when encountering large text files.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.sort_args">
<code class="descclassname">torchfun.</code><code class="descname">sort_args</code><span class="sig-paren">(</span><em>args_or_types</em>, <em>types_or_args</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.sort_args" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a very interesting function.
It is used to support __arbitrary-<a href="#id31"><span class="problematic" id="id33">arguments-ordering__</span></a> in TorchFun.</p>
<dl class="simple">
<dt>Input:</dt><dd><p>The function takes a list of types, and a list of arguments.</p>
</dd>
<dt>Returns:</dt><dd><p>a list of arguments, with the same order as the types-list.</p>
</dd>
</dl>
<p>Of course, <cite>sort_args</cite> supports arbitrary-arguments-ordering by itself.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.omini_open">
<code class="descclassname">torchfun.</code><code class="descname">omini_open</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.omini_open" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens everything using system default viwer.</p>
<p>This function can call system GUI to open folders,images,files,videos…</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.force_exist">
<code class="descclassname">torchfun.</code><code class="descname">force_exist</code><span class="sig-paren">(</span><em>dirname</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.force_exist" title="Permalink to this definition">¶</a></dt>
<dd><p>force a directory to exist.
force_exist can automatically create directory with any depth.
Arguements:</p>
<blockquote>
<div><p>dirname: path of the desired directory
verbose: print every directory creation. default True.</p>
</div></blockquote>
<dl class="simple">
<dt>Usage:</dt><dd><p>force_exist(‘a/b/c/d/e/f’)
force_exist(‘a/b/c/d/e/f’,verbose=False)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.to_numpy">
<code class="descclassname">torchfun.</code><code class="descname">to_numpy</code><span class="sig-paren">(</span><em>tensor</em>, <em>keep_dim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.to_numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>convert a NCHW tensor to NHWC numpy array.
if the input tensor has only one image,
then the N dimension will be deleted.</p>
<p>if the input is not 4-dimensional, 
the tensor will be simply cat to np array</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.dtype">
<code class="descclassname">torchfun.</code><code class="descname">dtype</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>return the data type of:</p>
<blockquote>
<div><p>a model</p>
<p>a tensor</p>
<p>or, the type() of anything else</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.imread">
<code class="descclassname">torchfun.</code><code class="descname">imread</code><span class="sig-paren">(</span><em>fname</em>, <em>out_range=(0</em>, <em>1)</em>, <em>dtype=torch.float32</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.imread" title="Permalink to this definition">¶</a></dt>
<dd><p>read jpg/png/gif/bmp/tiff… image file, and cat to tensor
function based on imageio.</p>
<p>args:</p>
<blockquote>
<div><p>fname: string of the file path</p>
<p>out_range: tuple, output pixel value range.</p>
<p>dtype: torch datatype</p>
</div></blockquote>
<dl class="simple">
<dt>Notice:</dt><dd><p>the returned image is 1xCxHxW (NCHW).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.imsave">
<code class="descclassname">torchfun.</code><code class="descname">imsave</code><span class="sig-paren">(</span><em>img_or_dest</em>, <em>dest_or_img</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.imsave" title="Permalink to this definition">¶</a></dt>
<dd><p>save torch tensor image(s) or numpy image(s).</p>
<p>img: can be numpy image, numpy image batch.</p>
<blockquote>
<div><p>or single torch-tensor image</p>
<p>or torch-tensor-image-batch.</p>
<p>or list/tuple of numpy images</p>
<p>or list/tuple of pytorch-tensor images</p>
</div></blockquote>
<p>Notice: images must have non-zero channels, even for grey-scale images (1-channel images).</p>
<dl class="simple">
<dt>behaviour: images will be concatenated into a long single image and save into destination file-name.</dt><dd><p>the concating will be taken horizontally.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.imwrite">
<code class="descclassname">torchfun.</code><code class="descname">imwrite</code><span class="sig-paren">(</span><em>img_or_dest</em>, <em>dest_or_img</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.imwrite" title="Permalink to this definition">¶</a></dt>
<dd><p>save torch tensor image(s) or numpy image(s).</p>
<p>img: can be numpy image, numpy image batch.</p>
<blockquote>
<div><p>or single torch-tensor image</p>
<p>or torch-tensor-image-batch.</p>
<p>or list/tuple of numpy images</p>
<p>or list/tuple of pytorch-tensor images</p>
</div></blockquote>
<p>Notice: images must have non-zero channels, even for grey-scale images (1-channel images).</p>
<dl class="simple">
<dt>behaviour: images will be concatenated into a long single image and save into destination file-name.</dt><dd><p>the concating will be taken horizontally.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.imshow">
<code class="descclassname">torchfun.</code><code class="descname">imshow</code><span class="sig-paren">(</span><em>x</em>, <em>title=None</em>, <em>auto_close=True</em>, <em>cols=8</em>, <em>backend=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.imshow" title="Permalink to this definition">¶</a></dt>
<dd><p>only deal with torch channel-first image batch,</p>
<dl class="simple">
<dt>title: add title to plot. (Default None)</dt><dd><p>title can be string, or any string-able object.</p>
</dd>
<dt>auto_close: (default True) </dt><dd><p>Close the pyplot session afterwards. 
Clean the environment just like you had 
never used matplotlib here.
if set to False, the plot will remain in the memory for further drawings.</p>
</dd>
<dt>cols: columns(default 8)</dt><dd><p>the width of the output grid, aka, number of images per row.</p>
</dd>
<dt>backend: None to use default gui. options are:</dt><dd><p>WebAgg,  GTK3Agg,
WX,      GTK3Cairo,
WXAgg,   MacOSX,
WXCairo, nbAgg,
agg,     Qt4Agg,
cairo,   Qt4Cairo,
pdf,     Qt5Agg,
pgf,     Qt5Cairo,
ps,      TkAgg,
svg,     TkCairo,
template</p>
</dd>
</dl>
<p>Usage:</p>
<dl class="simple">
<dt><a href="#id15"><span class="problematic" id="id16">``</span></a><a href="#id17"><span class="problematic" id="id18">`</span></a>python</dt><dd><p>imshow(batch)
imshow(batch,title=[a,b,c])
imshow(batch,title=’title’)
imshow(batch,auto_close=False)</p>
</dd>
</dl>
<p><a href="#id19"><span class="problematic" id="id20">``</span></a><a href="#id21"><span class="problematic" id="id22">`</span></a></p>
<p>Warnings:
<a href="#id23"><span class="problematic" id="id24">``</span></a><a href="#id25"><span class="problematic" id="id26">`</span></a>text</p>
<blockquote>
<div><p>TorchFun:imshow:Warning, you are using WebAgg backend for Matplotlib. 
Please consider windowed display SDKs such as TkAgg backend and GTK* backends.</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">This</span> <span class="pre">means</span> <span class="pre">your</span> <span class="pre">matplotlib</span> <span class="pre">is</span> <span class="pre">using</span> <span class="pre">web-browser</span> <span class="pre">for</span> <span class="pre">figure</span> <span class="pre">display.</span> <span class="pre">We</span> <span class="pre">__strongly__</span> <span class="pre">recommend</span> <span class="pre">you</span> <span class="pre">to</span> <span class="pre">use</span> <span class="pre">window-based</span> <span class="pre">native</span> <span class="pre">display</span> <span class="pre">because</span> <span class="pre">browser-based</span> <span class="pre">backends</span> <span class="pre">are</span> <span class="pre">fragile</span> <span class="pre">and</span> <span class="pre">tend</span> <span class="pre">to</span> <span class="pre">crash.</span> <span class="pre">You</span> <span class="pre">can</span> <span class="pre">change</span> <span class="pre">the</span> <span class="pre">display</span> <span class="pre">mamanger</span> <span class="pre">for</span> <span class="pre">matplotlib</span> <span class="pre">each</span> <span class="pre">time</span> <span class="pre">you</span> <span class="pre">execute</span> <span class="pre">your</span> <span class="pre">script</span> <span class="pre">by:</span>
<span class="pre">```python</span>
<span class="pre">import</span> <span class="pre">matplotlib</span>
<span class="pre">matplotlib.use('TkAgg')</span> <span class="pre">#</span> <span class="pre">or</span> <span class="pre">GTK</span> <span class="pre">GTKAgg</span>
<span class="pre">`</span></code>
or permanantly by editing: <cite>site-packages/matplotlib/mpl-data/matplotlibrc</cite> and change backend to <cite>TkAgg</cite></p>
<p>If your are using Unix-like systems such as MacOSX, you can create ~/.matplotlib/matplotlibrc and add a line: <cite>backend:TkAgg</cite> to it.</p>
<p>A full list of available backends can be found at:
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">import</span> <span class="pre">matplotlib</span>
<span class="pre">matplotlib.rcsetup.all_backends</span>
<span class="pre">`</span></code>
and, the TCL/TK GUI library for <cite>tkinter</cite> can be downloaded [here](<a class="reference external" href="https://www.tcl.tk/">https://www.tcl.tk/</a>).</p>
<dl>
<dt>Notice:</dt><dd><p>If you use conda to manage your python versions, errors may occur when using TCL/TK.
That’s because conda secretly redirect your global python library path towards its.
That will cause other stand-alone python versions to search from conda’s lib dirs for binaries.
To solve this, you may have to set:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">export</span> <span class="pre">TCL_LIBRARY=/usr/...pythondir.../lib/tcl8.6</span>
<span class="pre">export</span> <span class="pre">TK_LIBRARY=/usr/...pythondir.../lib/tcl8.6</span>
<span class="pre">`</span></code>
or on windows:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">set</span> <span class="pre">&quot;TCL_LIBRARY=/usr/...pythondir.../lib/tcl8.6&quot;</span>
<span class="pre">set</span> <span class="pre">&quot;TK_LIBRARY=/usr/...pythondir.../lib/tcl8.6&quot;</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.pil_imshow">
<code class="descclassname">torchfun.</code><code class="descname">pil_imshow</code><span class="sig-paren">(</span><em>arr</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.pil_imshow" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple showing of an image through an external viewer.</p>
<p>This function is only available if Python Imaging Library (PIL) is installed.</p>
<p>Uses the image viewer specified by the environment variable
SCIPY_PIL_IMAGE_VIEWER, or if that is not defined then <cite>see</cite>,
to view a temporary file generated from array data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function uses <cite>bytescale</cite> under the hood to rescale images to use
the full (0, 255) range if <code class="docutils literal notranslate"><span class="pre">mode</span></code> is one of <code class="docutils literal notranslate"><span class="pre">None,</span> <span class="pre">'L',</span> <span class="pre">'P',</span> <span class="pre">'l'</span></code>.
It will also cast data for 2-D images to <code class="docutils literal notranslate"><span class="pre">uint32</span></code> for <code class="docutils literal notranslate"><span class="pre">mode=None</span></code>
(which is the default).</p>
</div>
<dl class="simple">
<dt>arr<span class="classifier">ndarray</span></dt><dd><p>Array of image data to show.</p>
</dd>
</dl>
<p>None</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">misc</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">misc</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<p>Ported and upgraded based on scipy.misc.imshow
Open-sourced according to the license.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.imresize">
<code class="descclassname">torchfun.</code><code class="descname">imresize</code><span class="sig-paren">(</span><em>tensor_or_size</em>, <em>size_or_tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.imresize" title="Permalink to this definition">¶</a></dt>
<dd><p>stretch pytorch image NCHW, into given shape</p>
<p>arguments</p>
<blockquote>
<div><p>tensor: NCHW tensor</p>
<p>size: tuple or list of [height,width], or a scale factor</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.imcrop_center">
<code class="descclassname">torchfun.</code><code class="descname">imcrop_center</code><span class="sig-paren">(</span><em>tensor_or_size</em>, <em>size_or_tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.imcrop_center" title="Permalink to this definition">¶</a></dt>
<dd><p>crop a center patch from pytorch image NCHW</p>
<p>arguments</p>
<blockquote>
<div><p>tensor: NCHW tensor</p>
<dl class="simple">
<dt>size: tuple or list of [height,width], or a scale factor</dt><dd><p>given a tuple of height,width, this returns a scaled patch of the 
largest center crop.
given a factor of scale, this will return a scaled square maximum center crop</p>
</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.load">
<code class="descclassname">torchfun.</code><code class="descname">load</code><span class="sig-paren">(</span><em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load weight <cite>a</cite> into model <cite>b</cite>, or load model <cite>b</cite> using weight <cite>a</cite>
The order of the arguments doesn’t matter.</p>
<p>Example:</p>
<blockquote>
<div><p>&gt;load(‘weights.pts’,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;load(model,’weights.pts’)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;f = open(‘weight.pts’)</p>
<p>&gt;load(f,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;load(model,f)</p>
</div></blockquote>
<p>Return value: None</p>
<dl class="simple">
<dt>Behaviour: the loaded state-dict will be transformed to the same device as model,</dt><dd><p>so that torch won’t complain about CUDA-memory-insufficient when you just want to load
weights from disk directly to cpu-model.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.save">
<code class="descclassname">torchfun.</code><code class="descname">save</code><span class="sig-paren">(</span><em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.save" title="Permalink to this definition">¶</a></dt>
<dd><p>save weight <cite>a</cite> into target <cite>b</cite>, or save model <cite>b</cite> into target <cite>a</cite>
The order of the arguments doesn’t matter.
Example:</p>
<blockquote>
<div><p>&gt;save(‘weights.pts’,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(model,’weights.pts’)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;f = open(‘weight.pts’)
&gt;save(f,model)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(model,f)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(‘weights.pts’,state_dict)</p>
</div></blockquote>
<p>or</p>
<blockquote>
<div><p>&gt;save(state_dict,’weights.pts’)</p>
</div></blockquote>
<p>Return value: None</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.count_parameters">
<code class="descclassname">torchfun.</code><code class="descname">count_parameters</code><span class="sig-paren">(</span><em>model_or_dict_or_param</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.count_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Count parameter numer of a module/state_dict/layer/tensor.</p>
<p>This function can also print the occupied memory of parameters in MBs</p>
<p>Arguements:</p>
<p>model_or_dict_or_param: model or state dictionary or model.parameter(), or numpy-array, or tensor.</p>
<p>Return: parameter amount in python-int</p>
<blockquote>
<div><p>Returns 0 if datatype not understood</p>
</div></blockquote>
<p>Usage:</p>
<ol class="arabic">
<li><p>count trainable and untrainbale params</p>
<blockquote>
<div><p>count_parameters(model)</p>
</div></blockquote>
</li>
</ol>
<p>same as</p>
<blockquote>
<div><p>count_parameters(state_dict)</p>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>count only trainable params:</p>
<blockquote>
<div><p>count_parameters(model.parameters())</p>
</div></blockquote>
</li>
<li><p>count data matrix</p>
<blockquote>
<div><p>count_parameters(weight_tensor)</p>
<p>count_parameters(numpy_array)</p>
</div></blockquote>
</li>
</ol>
<p>Notice:</p>
<blockquote>
<div><p>return value is parameter Number.</p>
</div></blockquote>
<p>Alias: parameters()</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.parameters">
<code class="descclassname">torchfun.</code><code class="descname">parameters</code><span class="sig-paren">(</span><em>model_or_dict_or_param</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Count parameter numer of a module/state_dict/layer/tensor.</p>
<p>This function can also print the occupied memory of parameters in MBs</p>
<p>Arguements:</p>
<p>model_or_dict_or_param: model or state dictionary or model.parameter(), or numpy-array, or tensor.</p>
<p>Return: parameter amount in python-int</p>
<blockquote>
<div><p>Returns 0 if datatype not understood</p>
</div></blockquote>
<p>Usage:</p>
<ol class="arabic">
<li><p>count trainable and untrainbale params</p>
<blockquote>
<div><p>count_parameters(model)</p>
</div></blockquote>
</li>
</ol>
<p>same as</p>
<blockquote>
<div><p>count_parameters(state_dict)</p>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>count only trainable params:</p>
<blockquote>
<div><p>count_parameters(model.parameters())</p>
</div></blockquote>
</li>
<li><p>count data matrix</p>
<blockquote>
<div><p>count_parameters(weight_tensor)</p>
<p>count_parameters(numpy_array)</p>
</div></blockquote>
</li>
</ol>
<p>Notice:</p>
<blockquote>
<div><p>return value is parameter Number.</p>
</div></blockquote>
<p>Alias: parameters()</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.hash_parameters">
<code class="descclassname">torchfun.</code><code class="descname">hash_parameters</code><span class="sig-paren">(</span><em>model_or_statdict_or_param</em>, <em>use_sum=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.hash_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>return the summary of all variables.
This is used to detect chaotic changes of weights.
You can check the sum_parameters before and after some operations, to know
if there is any change made to the params.</p>
<p>I use this function to verify gradient behaviours.</p>
<p>By default, This only hash the trainable parameters!</p>
<p>arguements:</p>
<p>module_or_statdict_or_param: torch.nn.module,</p>
<blockquote>
<div><p>or model.state_dict(),</p>
<p>or model.parameters().</p>
</div></blockquote>
<p>use_sum: return the sum instead of mean value of all params.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.vectorize_parameter">
<code class="descclassname">torchfun.</code><code class="descname">vectorize_parameter</code><span class="sig-paren">(</span><em>model_or_statdict_or_param</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.vectorize_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>return the vectorized form of all variables.
This is used to detect chaotic changes of weights.</p>
<p>arguements:</p>
<p>module_or_statdict_or_param: torch.nn.module,</p>
<blockquote>
<div><p>or model.state_dict(),</p>
<p>or model.parameters().</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.show_layers_parameters">
<code class="descclassname">torchfun.</code><code class="descname">show_layers_parameters</code><span class="sig-paren">(</span><em>model</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.show_layers_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torchfun.show">
<code class="descclassname">torchfun.</code><code class="descname">show</code><span class="sig-paren">(</span><em>net</em>, <em>input_shape=(1</em>, <em>3</em>, <em>32</em>, <em>32)</em>, <em>logdir='tensorboard'</em>, <em>port=8888</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.show" title="Permalink to this definition">¶</a></dt>
<dd><p>print the network architecture on web-browser, using tensorboardX and tensorboard.
tensoboard must be install to use this tool.
this tool will create a noise data according to given input_shape,
and feed it directly to net, in order to probe its structure.
network strctures descriptions will be written to logdir.
a tensorboard daemon will be launched to read the logdir and start a web server
on given port.</p>
<p>Notice:</p>
<blockquote>
<div><p>input shape must be NCHW, following pytorch style.</p>
<p>This program overwrites the system argument lists (sys.argv)</p>
</div></blockquote>
</dd></dl>

<dl class="class">
<dt id="torchfun.Packsearch">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Packsearch</code><span class="sig-paren">(</span><em>module_object</em>, <em>auto_init=True</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Packsearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Search names inside a package.</p>
<p>Given an module object as input:</p>
<p>&gt; p = Packsearch(torch)</p>
<p>or</p>
<p>&gt; p = Packsearch(‘torch’)</p>
<p>the instance p provide p.search() method. So that you can</p>
<p>search everything inside this package</p>
<p>&gt; p.search(‘maxpoo’)</p>
<p>or simply</p>
<p>&gt; p(‘maxpoo’)</p>
<p>output:</p>
<blockquote>
<div><p>Packsearch: 35 results found:</p>
<p>————-results start————-</p>
<p>0        torch.nn.AdaptiveMaxPool1d</p>
<p>1        torch.nn.AdaptiveMaxPool2d</p>
<p>2        torch.nn.AdaptiveMaxPool3d</p>
<p>3        torch.nn.FractionalMaxPool2d</p>
<p>4        torch.nn.MaxPool1d</p>
<p>5        torch.nn.MaxPool2d</p>
<p>…</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.Packsearch.dynamic_traverse">
<code class="descname">dynamic_traverse</code><span class="sig-paren">(</span><em>mod</em>, <em>query</em>, <em>search_attributes=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Packsearch.dynamic_traverse" title="Permalink to this definition">¶</a></dt>
<dd><p>traverse the module and simultaneously search for queried name</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Packsearch.preprocess_names">
<code class="descname">preprocess_names</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Packsearch.preprocess_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Packsearch.search">
<code class="descname">search</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Packsearch.search" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Packsearch.traverse">
<code class="descname">traverse</code><span class="sig-paren">(</span><em>mod</em>, <em>search_attributes=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Packsearch.traverse" title="Permalink to this definition">¶</a></dt>
<dd><p>gather all names and store them into a name_list
search_attributes: whether to include class attributes or method names</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.packsearch">
<code class="descclassname">torchfun.</code><code class="descname">packsearch</code><span class="sig-paren">(</span><em>module_or_str</em>, <em>str_or_module</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.packsearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an module object, and search pattern string as input:</p>
<p>&gt; packsearch(torch,’maxpoo’)</p>
<p>or</p>
<p>&gt; packsearch(‘maxpoo’,torch)</p>
<p>you can search everything inside this package</p>
<p>output:</p>
<blockquote>
<div><p>Packsearch: 35 results found:</p>
<p>————-results start————-</p>
<p>0        torch.nn.AdaptiveMaxPool1d</p>
<p>1        torch.nn.AdaptiveMaxPool2d</p>
<p>2        torch.nn.AdaptiveMaxPool3d</p>
<p>3        torch.nn.FractionalMaxPool2d</p>
<p>4        torch.nn.MaxPool1d</p>
<p>5        torch.nn.MaxPool2d</p>
<p>…</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.whereis">
<code class="descclassname">torchfun.</code><code class="descname">whereis</code><span class="sig-paren">(</span><em>module_or_string</em>, <em>open_gui=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.whereis" title="Permalink to this definition">¶</a></dt>
<dd><p>find the source file location of a module</p>
<p>arguments:</p>
<blockquote>
<div><p>module_or_string: target module object, or it’s string path like <cite>torch.nn</cite></p>
<p>open_gui: open the folder with default window-manager.</p>
</div></blockquote>
<p>returns:</p>
<blockquote>
<div><p>module file name, or None</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.tf_session">
<code class="descclassname">torchfun.</code><code class="descname">tf_session</code><span class="sig-paren">(</span><em>allow_growth=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.tf_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to create tensorflow session that does not stupidly and unresonably consume all gpu-memeory.
returns:</p>
<blockquote>
<div><p>a tensorflow session consuming dynamic gpu memory.</p>
</div></blockquote>
</dd></dl>

<dl class="class">
<dt id="torchfun.Options">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Options</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Options" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
<p>A simple yet effective option class for debugging use.
key features: you can set attributes to it directly.
like:</p>
<blockquote>
<div><p>o = Options()
o.what=1
o.hahah=123</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.options">
<code class="descclassname">torchfun.</code><code class="descname">options</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.options" title="Permalink to this definition">¶</a></dt>
<dd><p>warpping class for Options. this function returns an option object with attributes
set according to the input key-value arguments. 
please refer to Option class for more information</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.Unimodel">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Unimodel</code><span class="sig-paren">(</span><em>*models</em>, <em>**named_models</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Unimodel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>this class is used to gather your multiple models, so that
you can save/load them together.</p>
<p>usage:</p>
<blockquote>
<div><p>unimodel = Unimodel(resnet1,resnet2,resnet3)</p>
<p>unimodel.save(‘xxxx.pth’)</p>
<p>unimodel.load(‘ssss.pth’)</p>
<p>resnet1 = unimodel.resnet1</p>
<p>…</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.Unimodel.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Unimodel.load" title="Permalink to this definition">¶</a></dt>
<dd><p>same as torchfun load()</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Unimodel.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Unimodel.save" title="Permalink to this definition">¶</a></dt>
<dd><p>same as torchfun save()</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.Bool">
<code class="descclassname">torchfun.</code><code class="descname">Bool</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Bool" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=torchfun.bool),
in order to parse bool switch values like:
false False true True 0 1</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.TorchEasyList">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">TorchEasyList</code><a class="headerlink" href="#torchfun.TorchEasyList" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.list" title="torchfun.list"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a></p>
<dl class="method">
<dt id="torchfun.TorchEasyList.cat">
<code class="descname">cat</code><span class="sig-paren">(</span><em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.TorchEasyList.cat" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.TorchEasyList.push_back">
<code class="descname">push_back</code><span class="sig-paren">(</span><em>element</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.TorchEasyList.push_back" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.TorchEasyList.push_head">
<code class="descname">push_head</code><span class="sig-paren">(</span><em>element</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.TorchEasyList.push_head" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="attribute">
<dt id="torchfun.List">
<code class="descclassname">torchfun.</code><code class="descname">List</code><a class="headerlink" href="#torchfun.List" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#torchfun.types.TorchEasyList" title="torchfun.types.TorchEasyList"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.types.TorchEasyList</span></code></a></p>
</dd></dl>

<dl class="function">
<dt id="torchfun.documentation">
<code class="descclassname">torchfun.</code><code class="descname">documentation</code><span class="sig-paren">(</span><em>search=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.documentation" title="Permalink to this definition">¶</a></dt>
<dd><p>help documentation on Torchfun
Argument:</p>
<blockquote>
<div><p>search: give None to go to the latest doc site</p>
<blockquote>
<div><p>give string or object to search the object</p>
</div></blockquote>
</div></blockquote>
</dd></dl>

<dl class="class">
<dt id="torchfun.Transform">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Transform</code><a class="headerlink" href="#torchfun.Transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>

<dl class="class">
<dt id="torchfun.RandomGaussianBlur">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomGaussianBlur</code><span class="sig-paren">(</span><em>kernel_ratio=0.01</em>, <em>random_ratio=0.005</em>, <em>pixel_range=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomGaussianBlur" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>PIL image</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.NoTransform">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">NoTransform</code><span class="sig-paren">(</span><em>*rubbish_args</em>, <em>**rubbish_kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NoTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="torchfun.Normalize">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Normalize</code><span class="sig-paren">(</span><em>mean</em>, <em>std</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Normalize a tensor image with mean and standard deviation.
Given mean: <code class="docutils literal notranslate"><span class="pre">(M1,...,Mn)</span></code> and std: <code class="docutils literal notranslate"><span class="pre">(S1,..,Sn)</span></code> for <code class="docutils literal notranslate"><span class="pre">n</span></code> channels, this transform
will normalize each channel of the input <code class="docutils literal notranslate"><span class="pre">torch.*Tensor</span></code> i.e.
<code class="docutils literal notranslate"><span class="pre">input[channel]</span> <span class="pre">=</span> <span class="pre">(input[channel]</span> <span class="pre">-</span> <span class="pre">mean[channel])</span> <span class="pre">/</span> <span class="pre">std[channel]</span></code></p>
<dl class="simple">
<dt>Args:</dt><dd><p>mean (sequence): Sequence of means for each channel.
std (sequence): Sequence of standard deviations for each channel.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.Resize">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Resize</code><span class="sig-paren">(</span><em>size</em>, <em>interpolation=2</em>, <em>interpolation_tg=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Resize" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Resize the input PIL Image to the given size.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size. If size is a sequence like</dt><dd><p>(h, w), output size will be matched to this. If size is an int,
smaller edge of the image will be matched to this number.
i.e, if height &gt; width, then image will be rescaled to
(size * height / width, size)</p>
</dd>
<dt>interpolation (int, optional): Desired interpolation. Default is</dt><dd><p><code class="docutils literal notranslate"><span class="pre">PIL.Image.BILINEAR</span></code></p>
</dd>
<dt>interpolation_tg (int, optional): Desired interpolation for target. Default is</dt><dd><p><code class="docutils literal notranslate"><span class="pre">PIL.Image.NEAREST</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.CenterCrop">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">CenterCrop</code><span class="sig-paren">(</span><em>size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.CenterCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crops the given PIL Image at the center.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an</dt><dd><p>int instead of sequence like (h, w), a square crop (size, size) is
made.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.RandomCrop">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomCrop</code><span class="sig-paren">(</span><em>size</em>, <em>padding=0</em>, <em>pad_if_needed=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image at a random location.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an</dt><dd><p>int instead of sequence like (h, w), a square crop (size, size) is
made.</p>
</dd>
<dt>padding (int or sequence, optional): Optional padding on each border</dt><dd><p>of the image. Default is 0, i.e no padding. If a sequence of length
4 is provided, it is used to pad left, top, right, bottom borders
respectively.</p>
</dd>
<dt>pad_if_needed (boolean): It will pad the image if smaller than the</dt><dd><p>desired size to avoid raising an exception.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.RandomCrop.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>img</em>, <em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomCrop.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for <code class="docutils literal notranslate"><span class="pre">crop</span></code> for a random crop.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>img (PIL Image): Image to be cropped.
output_size (tuple): Expected output size of the crop.</p>
</dd>
<dt>Returns:</dt><dd><p>tuple: params (i, j, h, w) to be passed to <code class="docutils literal notranslate"><span class="pre">crop</span></code> for random crop.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.RandomHorizontalFlip">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomHorizontalFlip</code><span class="sig-paren">(</span><em>p=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomHorizontalFlip" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Horizontally flip the given PIL Image randomly with a given probability.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p (float): probability of the image being flipped. Default value is 0.5</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.RandomVerticalFlip">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomVerticalFlip</code><span class="sig-paren">(</span><em>p=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomVerticalFlip" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Vertically flip the given PIL Image randomly with a given probability.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p (float): probability of the image being flipped. Default value is 0.5</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.RandomResizedCrop">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomResizedCrop</code><span class="sig-paren">(</span><em>size</em>, <em>scale=(0.08</em>, <em>1.0)</em>, <em>ratio=(0.75</em>, <em>1.3333333333333333)</em>, <em>interpolation=2</em>, <em>interpolation_tg=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomResizedCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image to random size and aspect ratio.</p>
<p>A crop of random size (default: of 0.08 to 1.0) of the original size and a random
aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop
is finally resized to given size.
This is popularly used to train the Inception networks.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>size: expected output size of each edge
scale: range of size of the origin size cropped
ratio: range of aspect ratio of the origin aspect ratio cropped
interpolation: Default: PIL.Image.BILINEAR</p>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.RandomResizedCrop.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>img</em>, <em>scale</em>, <em>ratio</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomResizedCrop.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for <code class="docutils literal notranslate"><span class="pre">crop</span></code> for a random sized crop.
Args:</p>
<blockquote>
<div><p>img (PIL Image): Image to be cropped.
scale (tuple): range of size of the origin size cropped
ratio (tuple): range of aspect ratio of the origin aspect ratio cropped</p>
</div></blockquote>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt>tuple: params (i, j, h, w) to be passed to <code class="docutils literal notranslate"><span class="pre">crop</span></code> for a random</dt><dd><p>sized crop.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.FiveCrop">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">FiveCrop</code><span class="sig-paren">(</span><em>size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.FiveCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image into four corners and the central crop</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This transform returns a tuple of images and there may be a mismatch in the number of
inputs and targets your Dataset returns. See below for an example of how to deal with
this.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an <code class="docutils literal notranslate"><span class="pre">int</span></code></dt><dd><p>instead of sequence like (h, w), a square crop of size (size, size) is made.</p>
</dd>
</dl>
</dd>
<dt>Example:</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">FiveCrop</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="c1"># this is a list of PIL Images</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">crops</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">crop</span><span class="p">)</span> <span class="k">for</span> <span class="n">crop</span> <span class="ow">in</span> <span class="n">crops</span><span class="p">]))</span> <span class="c1"># returns a 4D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#In your test loop you can do the following:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span> <span class="c1"># input is a 5d tensor, target is 2d</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span> <span class="c1"># fuse batch size and ncrops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result_avg</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># avg over crops</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.TenCrop">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">TenCrop</code><span class="sig-paren">(</span><em>size</em>, <em>vertical_flip=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.TenCrop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Crop the given PIL Image into four corners and the central crop plus the flipped version of
these (horizontal flipping is used by default)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This transform returns a tuple of images and there may be a mismatch in the number of
inputs and targets your Dataset returns. See below for an example of how to deal with
this.</p>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>size (sequence or int): Desired output size of the crop. If size is an</dt><dd><p>int instead of sequence like (h, w), a square crop (size, size) is
made.</p>
</dd>
</dl>
<p>vertical_flip(bool): Use vertical flipping instead of horizontal</p>
</dd>
<dt>Example:</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">TenCrop</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="c1"># this is a list of PIL Images</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">crops</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">crop</span><span class="p">)</span> <span class="k">for</span> <span class="n">crop</span> <span class="ow">in</span> <span class="n">crops</span><span class="p">]))</span> <span class="c1"># returns a 4D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#In your test loop you can do the following:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span> <span class="c1"># input is a 5d tensor, target is 2d</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span> <span class="c1"># fuse batch size and ncrops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result_avg</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">ncrops</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># avg over crops</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.LinearTransformation">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">LinearTransformation</code><span class="sig-paren">(</span><em>transformation_matrix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.LinearTransformation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Transform a tensor image with a square transformation matrix computed
offline.</p>
<p>Given transformation_matrix, will flatten the torch.*Tensor, compute the dot
product with the transformation matrix and reshape the tensor to its
original shape.</p>
<p>Applications:
- whitening: zero-center the data, compute the data covariance matrix</p>
<blockquote>
<div><p>[D x D] with np.dot(X.T, X), perform SVD on this matrix and
pass it as transformation_matrix.</p>
</div></blockquote>
<dl class="simple">
<dt>Args:</dt><dd><p>transformation_matrix (Tensor): tensor [D x D], D = C x H x W</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.ColorJitter">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">ColorJitter</code><span class="sig-paren">(</span><em>brightness=0</em>, <em>contrast=0</em>, <em>saturation=0</em>, <em>hue=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ColorJitter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Randomly change the brightness, contrast and saturation of an image.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>brightness (float): How much to jitter brightness. brightness_factor</dt><dd><p>is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].</p>
</dd>
<dt>contrast (float): How much to jitter contrast. contrast_factor</dt><dd><p>is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].</p>
</dd>
<dt>saturation (float): How much to jitter saturation. saturation_factor</dt><dd><p>is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].</p>
</dd>
<dt>hue(float): How much to jitter hue. hue_factor is chosen uniformly from</dt><dd><p>[-hue, hue]. Should be &gt;=0 and &lt;= 0.5.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.ColorJitter.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>brightness</em>, <em>contrast</em>, <em>saturation</em>, <em>hue</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ColorJitter.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a randomized transform to be applied on image.</p>
<p>Arguments are same as that of __init__.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Transform which randomly adjusts brightness, contrast and
saturation in a random order.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.RandomRotation">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomRotation</code><span class="sig-paren">(</span><em>degrees</em>, <em>resample=False</em>, <em>resample_tg=False</em>, <em>expand=False</em>, <em>center=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomRotation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Rotate the image by angle.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>degrees (sequence or float or int): Range of degrees to select from.</dt><dd><p>If degrees is a number instead of sequence like (min, max), the range of degrees
will be (-degrees, +degrees).</p>
</dd>
<dt>resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):</dt><dd><p>An optional resampling filter.
See <a class="reference external" href="http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters">http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters</a>
If omitted, or if the image has mode “1” or “P”, it is set to PIL.Image.NEAREST.</p>
</dd>
<dt>expand (bool, optional): Optional expansion flag.</dt><dd><p>If true, expands the output to make it large enough to hold the entire rotated image.
If false or omitted, make the output image the same size as the input image.
Note that the expand flag assumes rotation around the center and no translation.</p>
</dd>
<dt>center (2-tuple, optional): Optional center of rotation.</dt><dd><p>Origin is the upper left corner.
Default is the center of the image.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.RandomRotation.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>degrees</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomRotation.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for <code class="docutils literal notranslate"><span class="pre">rotate</span></code> for a random rotation.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>sequence: params to be passed to <code class="docutils literal notranslate"><span class="pre">rotate</span></code> for random rotation.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.RandomAffine">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomAffine</code><span class="sig-paren">(</span><em>degrees</em>, <em>translate=None</em>, <em>scale=None</em>, <em>shear=None</em>, <em>resample=False</em>, <em>resample_tg=False</em>, <em>fillcolor=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomAffine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Random affine transformation of the image keeping center invariant</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>degrees (sequence or float or int): Range of degrees to select from.</dt><dd><p>If degrees is a number instead of sequence like (min, max), the range of degrees
will be (-degrees, +degrees). Set to 0 to desactivate rotations.</p>
</dd>
<dt>translate (tuple, optional): tuple of maximum absolute fraction for horizontal</dt><dd><p>and vertical translations. For example translate=(a, b), then horizontal shift
is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a and vertical shift is
randomly sampled in the range -img_height * b &lt; dy &lt; img_height * b. Will not translate by default.</p>
</dd>
<dt>scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is</dt><dd><p>randomly sampled from the range a &lt;= scale &lt;= b. Will keep original scale by default.</p>
</dd>
<dt>shear (sequence or float or int, optional): Range of degrees to select from.</dt><dd><p>If degrees is a number instead of sequence like (min, max), the range of degrees
will be (-degrees, +degrees). Will not apply shear by default</p>
</dd>
<dt>resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):</dt><dd><p>An optional resampling filter.
See <a class="reference external" href="http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters">http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters</a>
If omitted, or if the image has mode “1” or “P”, it is set to PIL.Image.NEAREST.</p>
</dd>
</dl>
<p>fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow&gt;=5.0.0)</p>
</dd>
</dl>
<dl class="staticmethod">
<dt id="torchfun.RandomAffine.get_params">
<em class="property">static </em><code class="descname">get_params</code><span class="sig-paren">(</span><em>degrees</em>, <em>translate</em>, <em>scale_ranges</em>, <em>shears</em>, <em>img_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomAffine.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for affine transformation</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>sequence: params to be passed to the affine transformation</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Grayscale">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Grayscale</code><span class="sig-paren">(</span><em>num_output_channels=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Grayscale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Convert image to grayscale.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>num_output_channels (int): (1 or 3) number of channels desired for output image</p>
</dd>
<dt>Returns:</dt><dd><p>PIL Image: Grayscale version of the input.
- If num_output_channels == 1 : returned image is single channel
- If num_output_channels == 3 : returned image is 3 channel with r == g == b</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.RandomGrayscale">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">RandomGrayscale</code><span class="sig-paren">(</span><em>p=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.RandomGrayscale" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.transforms.Transform" title="torchfun.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.transforms.Transform</span></code></a></p>
<p>Randomly convert image to grayscale with a probability of p (default 0.1).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>p (float): probability that image should be converted to grayscale.</p>
</dd>
<dt>Returns:</dt><dd><p>PIL Image: Grayscale version of the input image with probability p and unchanged
with probability (1-p).
- If input image is 1 channel: grayscale version is 1 channel
- If input image is 3 channel: grayscale version is 3 channel with r == g == b</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.conv2d_dfs">
<code class="descclassname">torchfun.</code><code class="descname">conv2d_dfs</code><span class="sig-paren">(</span><em>x</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.conv2d_dfs" title="Permalink to this definition">¶</a></dt>
<dd><p>depth fully shared conv2d.
Argument:</p>
<blockquote>
<div><p>x: input image with size: N x C x H x W
weight: shape shoule be : 1 x 1 x kernel-height x k-width
bias: contains only 1 number or None</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.flatten">
<code class="descclassname">torchfun.</code><code class="descname">flatten</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flatten function
Usage:</p>
<blockquote>
<div><p>out = flatten(x)</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.subpixel">
<code class="descclassname">torchfun.</code><code class="descname">subpixel</code><span class="sig-paren">(</span><em>x</em>, <em>out_channels=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.subpixel" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfold channel/depth dimensions to enlarge the feature map
Notice: Output size is deducted. 
The size of the unfold square is automatically determined
e.g. :</p>
<blockquote>
<div><p>images: 100x9x16x16.  9=3x3 square
subpixel-out: 100x1x48x48</p>
</div></blockquote>
<dl class="simple">
<dt>Arguement:</dt><dd><p>x: NCHW image, channel first.
out_channels, channel number of output feature map</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.clip">
<code class="descclassname">torchfun.</code><code class="descname">clip</code><span class="sig-paren">(</span><em>in_tensor</em>, <em>max_or_min</em>, <em>min_or_max</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.clip" title="Permalink to this definition">¶</a></dt>
<dd><p>limit the values in in_tensor to be within [min,max].
values larger than max will be cut to max, respectively for mins.
the order of max/min doesn’t matter.
the operation is not in-place, that saves you alot troubles.
Notice: this clip is not inplace, and neither can pass backward derivatives</p>
<blockquote>
<div><p>when the values are clipped to min/max.</p>
</div></blockquote>
<dl class="simple">
<dt>Warning: When applied during loss-calculating in training, this clip will cause</dt><dd><p>gradient disapperance.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.clip_">
<code class="descclassname">torchfun.</code><code class="descname">clip_</code><span class="sig-paren">(</span><em>in_tensor</em>, <em>max_or_min</em>, <em>min_or_max</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.clip_" title="Permalink to this definition">¶</a></dt>
<dd><p>limit the values in in_tensor to be within [min,max].
values larger than max will be cut to max, respectively for mins.
the order of max/min doesn’t matter.
the operation is not in-place, that saves you alot troubles.
Notice: this clip is not inplace.</p>
<blockquote>
<div><p>But, for clipped values, the gradients will always be passed backwards.
This is useful when you want <cite>clip</cite> as a value-normalizer, but don not want the gradient to vanish.</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.add_noise">
<code class="descclassname">torchfun.</code><code class="descname">add_noise</code><span class="sig-paren">(</span><em>in_tensor</em>, <em>noise_type='normal'</em>, <em>noise_param=(0</em>, <em>1)</em>, <em>range_limit=(-1</em>, <em>1)</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.add_noise" title="Permalink to this definition">¶</a></dt>
<dd><p>Add noise to input tensor.
Noise type can be either <cite>normal</cite> or <cite>uniform</cite></p>
<blockquote>
<div><ul class="simple">
<li><p>for normal, (mean,std) is required as noise_param</p></li>
<li><p>for uniform (min,max) is required as noise_param</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>The range of the output tensor can be limited,</dt><dd><p>by giving <cite>range_limit</cite>:(min,max)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.instance_mean_std">
<code class="descclassname">torchfun.</code><code class="descname">instance_mean_std</code><span class="sig-paren">(</span><em>x</em>, <em>num_features</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.instance_mean_std" title="Permalink to this definition">¶</a></dt>
<dd><p>NCHW</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.instance_renorm">
<code class="descclassname">torchfun.</code><code class="descname">instance_renorm</code><span class="sig-paren">(</span><em>x</em>, <em>mean</em>, <em>std</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.instance_renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Make x have given mean and std.
Argument:
x: NCHW
mean: tensor with size: N-by-num-features
std: tensor with size: N-by-num-features
eps: default 1e-5</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.max_min_norm">
<code class="descclassname">torchfun.</code><code class="descname">max_min_norm</code><span class="sig-paren">(</span><em>x</em>, <em>to_max</em>, <em>to_min</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.max_min_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>scale the input x into range [to_min,to_max].
Arguments:</p>
<blockquote>
<div><p>to_max: target expect max value of the output
to_min: target expect min value of the output</p>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torchfun.combine_parameters">
<code class="descclassname">torchfun.</code><code class="descname">combine_parameters</code><span class="sig-paren">(</span><em>*models</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.combine_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Combine the parameters of serveral trainable module object,
into one unified parameter generator.
Arguements:</p>
<blockquote>
<div><p><a href="#id27"><span class="problematic" id="id28">*</span></a>models: any number of models.</p>
</div></blockquote>
<p>This utility is useful when you want several individual parts to be
handled by one Optimizer. Parameters shall be gathered into one iterator
first, because torch.optimizers require only one parameter-iterator as input</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.cosine_similarity">
<code class="descclassname">torchfun.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>compute cosine similarity between two batch of data.
x1 x2 are flattened first, into N x V shape.
then each paired sample between x1 and x2 are used to compute a 
multi-dimensional vectorial cosine similarity.</p>
<p>output:  similarity output with size N, which is a vector.</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.time">
<code class="descclassname">torchfun.</code><code class="descname">time</code><span class="sig-paren">(</span><em>message=None, name=None, named_time_table={}, anonymous_time=[None]</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.time" title="Permalink to this definition">¶</a></dt>
<dd><p>show running time
dict-key locating costs 1e-6 second = 1 micro-second
context switching costs 5e-5 second.</p>
<dl class="simple">
<dt>Usage 1:</dt><dd><p>tf.time()
…
elapsed = tf.time()</p>
</dd>
<dt>Usage 2:</dt><dd><p>tf.time()
…
tf.time(‘elapsed:’)
out: elapsed: 0.02 sec</p>
</dd>
<dt>Usage 3:</dt><dd><p>tf.time(name=’clock1’)
…
tf.time(‘elapsed’,name=’clock1’)
out: elapsed 0.02 sec</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torchfun.OrderedDict">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">OrderedDict</code><a class="headerlink" href="#torchfun.OrderedDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
<p>Dictionary that remembers insertion order</p>
<dl class="method">
<dt id="torchfun.OrderedDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None.  Remove all items from od.<a class="headerlink" href="#torchfun.OrderedDict.clear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; a shallow copy of od<a class="headerlink" href="#torchfun.OrderedDict.copy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.fromkeys">
<code class="descname">fromkeys</code><span class="sig-paren">(</span><em>S</em><span class="optional">[</span>, <em>v</em><span class="optional">]</span><span class="sig-paren">)</span> &#x2192; New ordered dictionary with keys from S.<a class="headerlink" href="#torchfun.OrderedDict.fromkeys" title="Permalink to this definition">¶</a></dt>
<dd><p>If not specified, the value defaults to None.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; a set-like object providing a view on D's items<a class="headerlink" href="#torchfun.OrderedDict.items" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; a set-like object providing a view on D's keys<a class="headerlink" href="#torchfun.OrderedDict.keys" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.move_to_end">
<code class="descname">move_to_end</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.OrderedDict.move_to_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Move an existing element to the end (or beginning if last==False).</p>
<p>Raises KeyError if the element does not exist.
When last=True, acts like a fast version of self[key]=self.pop(key).</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>k</em><span class="optional">[</span>, <em>d</em><span class="optional">]</span><span class="sig-paren">)</span> &#x2192; v, remove specified key and return the corresponding<a class="headerlink" href="#torchfun.OrderedDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>value.  If key is not found, d is returned if given, otherwise KeyError
is raised.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.popitem">
<code class="descname">popitem</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.OrderedDict.popitem" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove and return a (key, value) pair from the dictionary.</p>
<p>Pairs are returned in LIFO order if last is true or FIFO order if false.</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.setdefault">
<code class="descname">setdefault</code><span class="sig-paren">(</span><em>k</em><span class="optional">[</span>, <em>d</em><span class="optional">]</span><span class="sig-paren">)</span> &#x2192; od.get(k,d), also set od[k]=d if k not in od<a class="headerlink" href="#torchfun.OrderedDict.setdefault" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><span class="optional">[</span><em>E</em>, <span class="optional">]</span><em>**F</em><span class="sig-paren">)</span> &#x2192; None.  Update D from dict/iterable E and F.<a class="headerlink" href="#torchfun.OrderedDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
In either case, this is followed by: for k in F:  D[k] = F[k]</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.OrderedDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; an object providing a view on D's values<a class="headerlink" href="#torchfun.OrderedDict.values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.DebugAgent">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">DebugAgent</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.DebugAgent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>wrapper around layers.
this wrapper hooks the forward function, to measure time 
consumption of this layer.</p>
<dl class="method">
<dt id="torchfun.DebugAgent.bind">
<code class="descname">bind</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.DebugAgent.bind" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Module">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Module</code><a class="headerlink" href="#torchfun.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>More debugging/controlling methods with complete original
features from torch.nn.Module
provides:</p>
<blockquote>
<div><ul class="simple">
<li><p>debug mode: inspect running time layer by layer.</p></li>
<li><p>release mode: back to normal from debug mode.</p></li>
<li><p>freeze layers: set layers to be in-trainable</p></li>
<li><p>unfreeze: as the name implies.</p></li>
<li><p>unfreeze_all: so as the name implies.</p></li>
</ul>
</div></blockquote>
<dl>
<dt>Notice: you can safely change the base class from torchfun module</dt><dd><p>back to torch module, when you want to publish the model.
the state_dict will be loaded correctly and the forward() will
function the same.</p>
</dd>
<dt>Hint:   Consider establishing a BaseClass global variable at the top of your </dt><dd><p>code. Use a argument parser to select between torchfun.nn.Module and torch.nn.Module,
so that the following classes follows the specified base class</p>
<dl>
<dt>Example::</dt><dd><div class="line-block">
<div class="line">import torch.nn.Module as ReleaseModule</div>
<div class="line">import torchfun.nn.Module as DevModule    </div>
<div class="line">from sys import argv</div>
<div class="line">if argv[1] == ‘develop’:</div>
<div class="line-block">
<div class="line">Base = DevModule</div>
</div>
<div class="line">elif argv[1] == ‘release’:</div>
<div class="line-block">
<div class="line">Base = ReleaseModule</div>
<div class="line"><br /></div>
</div>
<div class="line">class MyModel(Base):</div>
<div class="line-block">
<div class="line">…</div>
</div>
</div>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="torchfun.Module.debug">
<code class="descname">debug</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.debug" title="Permalink to this definition">¶</a></dt>
<dd><p>turn on debug mode.
allow detailed timing report of forward()</p>
</dd></dl>

<dl class="method">
<dt id="torchfun.Module.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><em>*obj_or_name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.freeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>Parameter: module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torchfun.Module.release">
<code class="descname">release</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.release" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torchfun.Module.unfreeze">
<code class="descname">unfreeze</code><span class="sig-paren">(</span><em>*obj_or_name</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Module.unfreeze_all">
<code class="descname">unfreeze_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Module.unfreeze_all" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Flatten">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Flatten</code><a class="headerlink" href="#torchfun.Flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Flatten module
Usage:</p>
<blockquote>
<div><p>flat = Flatten()
out = flat(x)</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.Flatten.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Flatten.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Subpixel">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Subpixel</code><span class="sig-paren">(</span><em>out_channels=1</em>, <em>stride=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Subpixel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Unfold channel/depth dimensions to enlarge the feature map
Notice: Output size is deducted. 
The size of the unfold square is automatically determined
e.g. :</p>
<blockquote>
<div><p>images: 100x16x16x9.  9=3x3 square
subpixel-out: 100x48x48x1</p>
</div></blockquote>
<dl class="simple">
<dt>Arguement:</dt><dd><p>out_channels, channel number of output feature map
stride: enlarging ratio of spatial dimensions. stride=2 outputs x4 img area. If provided, out_channels will be ignored.</p>
</dd>
</dl>
<dl class="method">
<dt id="torchfun.Subpixel.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Subpixel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Conv2dDepthShared">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Conv2dDepthShared</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>trunks</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Conv2dDepthShared" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
<p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>Share the kernel along depth/channel direction.
Conv2dDepthShared divides input images into multiple sub-layers(trunks) along depth axis, and use shared kernel to process each depth trunk.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{in}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\begin{equation*}
\text{out}(N_i, C_{out_j}) = \text{bias}(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{out_j}, k) \star \text{input}(N_i, k)
\end{equation*},\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels. 
The <cite>weight</cite> and <cite>bias</cite> matrix of a Conv2dDepthShared are low-rank. 
They share trunks of digits repeatitively inside their matrices.</p>
<dl class="simple">
<dt>Example:</dt><dd><p>Conv(in=3,out=9,k=3,s=1) will create kernel weight with size of (9x3x5x5)
kernel of a Depth-shared-Conv2d only has 3x1x5x5 parameters.</p>
</dd>
</dl>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\left\lfloor\frac{\text{out_channels}}{\text{in_channels}}\right\rfloor\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The configuration when <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>
where <cite>K</cite> is a positive integer is termed in literature as depthwise convolution.</p>
<p>In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>, if you want a
depthwise convolution with a depthwise multiplier <cite>K</cite>,
then you use the constructor arguments
<span class="math notranslate nohighlight">\((\text{in_channels}=C_{in}, \text{out_channels}=C_{in} * K, ..., \text{groups}=C_{in})\)</span></p>
</div>
<dl>
<dt>Args:</dt><dd><p>in_channels (int): Number of channels in the input image, inchannels must can be divided by trunks.
out_channels (int): Number of channels produced by the convolution. out channels must can be divided by trunks.
trunks (int): Number of trunks a single image is divided into (along depth). All trunks inside an image share same weight/bias.
kernel_size (int or tuple): Size of the convolving kernel
stride (int or tuple, optional): Stride of the convolution. Default: 1
padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
bias (bool, optional): If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}H_{out} = \left\lfloor\frac{H_{in}  + 2 * \text{padding}[0] - \text{dilation}[0]
          * (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\\W_{out} = \left\lfloor\frac{W_{in}  + 2 * \text{padding}[1] - \text{dilation}[1]
          * (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\end{aligned}\end{align} \]</div>
</li>
</ul>
</dd>
<dt>Attributes:</dt><dd><dl class="simple">
<dt>weight (Tensor): the learnable weights of the module of shape</dt><dd><p>(out_channels, in_channels, kernel_size[0], kernel_size[1])</p>
</dd>
</dl>
<p>bias (Tensor):   the learnable bias of the module of shape (out_channels)</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="torchfun.Conv2dDepthShared.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Conv2dDepthShared.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Squeeze">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Squeeze</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>squeeze(dim=None) -&gt; Tensor</p>
<p>Returns a tensor with all the dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of size <cite>1</cite> removed.</p>
<p>For example, if <cite>input</cite> is of shape:
<span class="math notranslate nohighlight">\((A   imes 1  imes B  imes C  imes 1  imes D)\)</span> then the <cite>out</cite> tensor
will be of shape: <span class="math notranslate nohighlight">\((A         imes B  imes C  imes D)\)</span>.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is given, a squeeze operation is done only in the given
dimension. If <cite>input</cite> is of shape: <span class="math notranslate nohighlight">\((A        imes 1  imes B)\)</span>,
<cite>squeeze(input, 0)</cite> leaves the tensor unchanged, but <code class="xref py py-func docutils literal notranslate"><span class="pre">squeeze(input,</span> <span class="pre">1)()</span></code> will
squeeze the tensor to the shape <span class="math notranslate nohighlight">\((A   imes B)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As an exception to the above, a 1-dimensional tensor of size 1 will
not have its dimensions changed.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned tensor shares the storage with the input tensor,
so changing the contents of one will change the contents of the other.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 1, 2, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 2, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 1, 2, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 2, 1, 2]) </span>
</pre></div>
</div>
<dl class="method">
<dt id="torchfun.Squeeze.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Squeeze.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Clip">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Clip</code><span class="sig-paren">(</span><em>max_or_min</em>, <em>min_or_max</em>, <em>dtype=torch.float32</em>, <em>keep_grad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Clip" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>limit the values in in_tensor to be within [min,max].
values larger than max will be cut to max, respectively for mins.
the order of max/min doesn’t matter.
the operation is not in-place, that saves you alot troubles.
Notice: this clip is not inplace.</p>
<blockquote>
<div><p>But, for clipped values, the gradients will always be passed backwards.
This is useful when you want <cite>clip</cite> as a value-normalizer, but don not want the gradient to vanish.</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.Clip.forward_clip_grad">
<code class="descname">forward_clip_grad</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Clip.forward_clip_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torchfun.Clip.forward_keep_grad">
<code class="descname">forward_keep_grad</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Clip.forward_keep_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.NO_OP">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">NO_OP</code><span class="sig-paren">(</span><em>*argv</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NO_OP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A Module that repersents NO-Operation NO-OP.
NO-OP is needed when programmers want customizable dynamic assembling
of models. 
To disable some layers, instead of using multiple <cite>if</cite> clauses, nn-parts can be configured to be
NO-OP class, which will make that part turned-off in all occurrence.</p>
<p>Notice: NO_OP will accept any init-args, and ignore them.</p>
<dl class="staticmethod">
<dt id="torchfun.NO_OP.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>*argv</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.NO_OP.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.InstanceMeanStd">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">InstanceMeanStd</code><span class="sig-paren">(</span><em>num_features</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.InstanceMeanStd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>NCHW</p>
<dl class="method">
<dt id="torchfun.InstanceMeanStd.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.InstanceMeanStd.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>NCHW</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.InstanceReNorm">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">InstanceReNorm</code><span class="sig-paren">(</span><em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.InstanceReNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Make x have given mean and std.
Argument:
x: NCHW
mean: tensor with size: N-by-num-features
std: tensor with size: N-by-num-features
eps: default 1e-5</p>
<dl class="method">
<dt id="torchfun.InstanceReNorm.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>mean</em>, <em>std</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.InstanceReNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Make x have given mean and std.
Argument:
x: NCHW
mean: tensor with size: N-by-num-features
std: tensor with size: N-by-num-features
eps: default 1e-5</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.MaxMinNorm">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">MaxMinNorm</code><span class="sig-paren">(</span><em>max_or_min</em>, <em>min_or_max</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.MaxMinNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torchfun.nn.Clip" title="torchfun.nn.Clip"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.nn.Clip</span></code></a></p>
<p>scale the input x into range [to_min,to_max].
Arguments:</p>
<blockquote>
<div><p>to_max: target expect max value of the output
to_min: target expect min value of the output</p>
</div></blockquote>
<dl class="method">
<dt id="torchfun.MaxMinNorm.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.MaxMinNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.ReLU">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>*args</em>, <em>inplace=False</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.ReLU</span></code></p>
<p>activation that accepts any argument and ignores them.
useful when you want to switch between different activations
programatically,</p>
</dd></dl>

<dl class="class">
<dt id="torchfun.ReLUwithGrad">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">ReLUwithGrad</code><span class="sig-paren">(</span><em>*args</em>, <em>**kws</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ReLUwithGrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>a relu function with gradient not clipped to zero.
the gradient w.r.t input is not zero even when the input
is negative value.</p>
<p>This RELU output the value like a normal relu max(0,input)
but it output backward() like a linear function y=x,
dReLU/dx = 1</p>
<dl class="method">
<dt id="torchfun.ReLUwithGrad.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.ReLUwithGrad.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torchfun.Conv2dDepthFullyShared">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Conv2dDepthFullyShared</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Conv2dDepthFullyShared" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
</dd></dl>

<dl class="class">
<dt id="torchfun.Interpolate">
<em class="property">class </em><code class="descclassname">torchfun.</code><code class="descname">Interpolate</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em>, <em>mode='bilinear'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>resizing/scaling multi dimensional tensors
The modes available for resizing are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only), <cite>area</cite></p>
<dl>
<dt>Args:</dt><dd><p>input (Tensor): the input tensor
size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</p>
<blockquote>
<div><p>output spatial size.</p>
</div></blockquote>
<p>scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.
mode (string): algorithm used for upsampling:</p>
<blockquote>
<div><p>‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. Default: ‘nearest’</p>
</div></blockquote>
<dl class="simple">
<dt>align_corners (bool, optional): if True, the corner pixels of the input</dt><dd><p>and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code> for concrete examples on how this
affects the outputs.</p>
</div>
<dl class="method">
<dt id="torchfun.Interpolate.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.Interpolate.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torchfun.Module" title="torchfun.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torchfun.record_experiment">
<code class="descclassname">torchfun.</code><code class="descname">record_experiment</code><span class="sig-paren">(</span><em>exp_dir='not-specified'</em>, <em>record_top_dir='records'</em>, <em>logfilename='record.txt'</em>, <em>comment=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.record_experiment" title="Permalink to this definition">¶</a></dt>
<dd><p>Arguments:
* exp_dir : directory of this experiment output files,</p>
<blockquote>
<div><p>recorded so that it would be easier for you to find the outcome files 
of this experiment later.</p>
</div></blockquote>
<ul class="simple">
<li><dl class="simple">
<dt>record_top_dir<span class="classifier">create a dir to save all kinds of record logs. default(records). </span></dt><dd><p>set to empty string(‘’) to save all record in the current dir.</p>
</dd>
</dl>
</li>
<li><p>logfilename : filename of this log, usually <cite>train_record</cite> <cite>evaluation_record</cite> etc.</p></li>
<li><p>comment :string comment added to log paragraph. default is empty string.</p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="torchfun.cpu_memory">
<code class="descclassname">torchfun.</code><code class="descname">cpu_memory</code><span class="sig-paren">(</span><em>print_on_screen=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.cpu_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>total CPU memory usage of current python session.
returns: (RSS,VMS) In bytes!</p>
<blockquote>
<div><p>RSS is resident set size, 
VMS is virtual memory size.</p>
</div></blockquote>
<dl class="simple">
<dt>Notice: </dt><dd><p>return values are in bytes.
printed values are in MBs.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torchfun.change_suffix">
<code class="descclassname">torchfun.</code><code class="descname">change_suffix</code><span class="sig-paren">(</span><em>fpath</em>, <em>new_suffix</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.change_suffix" title="Permalink to this definition">¶</a></dt>
<dd><p>change the suffix of fpath to new_suffix.
if the original path fpath has no suffix, then new_suffix will be
directly applied at the end of the fpath string, joint by separator <cite>.</cite></p>
<p>In : change_suffix(‘xxx/ddd/www.x’,’aa’)
Out: ‘xxx/ddd/www.aa’</p>
<p>In : change_suffix(‘xxx/ddd/www’,’aa’)
Out: ‘xxx/ddd/www.aa’</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.change_fname">
<code class="descclassname">torchfun.</code><code class="descname">change_fname</code><span class="sig-paren">(</span><em>fpath</em>, <em>new_fname</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.change_fname" title="Permalink to this definition">¶</a></dt>
<dd><p>change the name of file in the given fpath.
the suffix (if exists) separated by <cite>.</cite> will be kept.</p>
<p>In : change_fname(‘xxx/ddd/www’,’aa’)
Out: ‘xxxdddaa’</p>
<p>In : change_fname(‘xxx/ddd/www.txt’,’aa’)
Out: ‘xxxdddaa.txt’</p>
</dd></dl>

<dl class="function">
<dt id="torchfun.reset_timer">
<code class="descclassname">torchfun.</code><code class="descname">reset_timer</code><span class="sig-paren">(</span><em>named_time_table={}, anonymous_time=[None]</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.reset_timer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torchfun.list">
<code class="descclassname">torchfun.</code><code class="descname">list</code><a class="headerlink" href="#torchfun.list" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#torchfun.types.TorchEasyList" title="torchfun.types.TorchEasyList"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.types.TorchEasyList</span></code></a></p>
</dd></dl>

<dl class="function">
<dt id="torchfun.bool">
<code class="descclassname">torchfun.</code><code class="descname">bool</code><span class="sig-paren">(</span><em>option_string</em><span class="sig-paren">)</span><a class="headerlink" href="#torchfun.bool" title="Permalink to this definition">¶</a></dt>
<dd><p>used to put into argparse add_argument(type=torchfun.bool),
in order to parse bool switch values like:
false False true True 0 1</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="descclassname">torchfun.</code><code class="descname">list</code></dt>
<dd><p>alias of <a class="reference internal" href="#torchfun.types.TorchEasyList" title="torchfun.types.TorchEasyList"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchfun.types.TorchEasyList</span></code></a></p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">torchfun package</a><ul>
<li><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-torchfun.datasets">torchfun.datasets module</a></li>
<li><a class="reference internal" href="#module-torchfun.functional">torchfun.functional module</a></li>
<li><a class="reference internal" href="#module-torchfun.metrics">torchfun.metrics module</a></li>
<li><a class="reference internal" href="#module-torchfun.nn">torchfun.nn module</a></li>
<li><a class="reference internal" href="#module-torchfun.path">torchfun.path module</a></li>
<li><a class="reference internal" href="#module-torchfun.torchfun">torchfun.torchfun module</a></li>
<li><a class="reference internal" href="#module-torchfun.transforms">torchfun.transforms module</a></li>
<li><a class="reference internal" href="#module-torchfun.types">torchfun.types module</a></li>
<li><a class="reference internal" href="#module-torchfun.ui">torchfun.ui module</a></li>
<li><a class="reference internal" href="#module-torchfun.utils">torchfun.utils module</a></li>
<li><a class="reference internal" href="#module-torchfun">Module contents</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/torchfun.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">torchfun 1.0.82 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, chensiyu.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.0.1.
    </div>
  </body>
</html>